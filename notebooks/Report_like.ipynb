{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Journey Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Transforming Data (Connecting Stops etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Creating Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Finding the Short Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Finding Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\":\"oh-my-git_final\",\n",
    "     \"driverMemory\": \"2000M\",\n",
    "    \"executorMemory\": \"8G\",\n",
    "    \"executorCores\": 4,\n",
    "    \"numExecutors\": 20\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from geopy import distance\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions  as F\n",
    "from pyspark.sql.window import Window\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from itertools import islice, tee, izip, combinations\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Some constants\n",
    "zurichHB = (47.378177, 8.540192)\n",
    "cleveland_oh = (49.378177, 9.540192)\n",
    "print(distance.distance(zurichHB, cleveland_oh).m)\n",
    "\n",
    "# Useful user-defined functions\n",
    "@F.udf(T.DoubleType())\n",
    "def getDistToZurich(lat,lon):\n",
    "    \"\"\"\n",
    "    Get distance to the Zurich HB in kilometers.\n",
    "    \"\"\"\n",
    "    zurichHB = (47.378177, 8.540192)\n",
    "    geo = (lat,lon)\n",
    "    return distance.distance(zurichHB, geo).km\n",
    "\n",
    "@F.udf(T.ArrayType(\n",
    "       T.ArrayType(T.StructType([\n",
    "    T.StructField(\"pair1\", T.StringType(), False),\n",
    "    T.StructField(\"pair2\", T.StringType(), False)\n",
    "]))))\n",
    "def pairs(stop_ids,stop_sequences,arrival_times,departure_times):\n",
    "    \"\"\"\n",
    "    Print the journey information in pairs.\n",
    "    \"\"\"\n",
    "    stop_id_pairs = list(combinations(stop_ids,2))\n",
    "    stop_sequence_pairs = list(combinations(stop_sequences,2))\n",
    "    arrival_time_pairs = list(combinations(arrival_times,2))\n",
    "    departure_time_pairs = list(combinations(departure_times,2))\n",
    "    return [stop_id_pairs,stop_sequence_pairs,arrival_time_pairs,departure_time_pairs]\n",
    "\n",
    "@F.udf(T.BooleanType())\n",
    "def time_interval_udf(arrival_time_schedule,transfer_arrival_time):\n",
    "    \"\"\"\n",
    "    Get a 2 minute time interval in order to do find relevant vehicles in the SBB data.\n",
    "    \"\"\"\n",
    "    if arrival_time_schedule == \"\":\n",
    "        return False\n",
    "    ah = transfer_arrival_time.split(\":\")[0]\n",
    "    am = transfer_arrival_time.split(\":\")[1]\n",
    "    a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "    \n",
    "    atsh = arrival_time_schedule.split(\":\")[0]\n",
    "    atsm = arrival_time_schedule.split(\":\")[1]\n",
    "    ats = datetime.datetime(2019, 1, 1, int(atsh), int(atsm))\n",
    "    \n",
    "    #Allow 120 seconds time difference\n",
    "    if abs(ats-a).total_seconds() <= 120:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "@F.udf(T.DoubleType())\n",
    "def getDist(lat1,lon1,lat2,lon2):\n",
    "    \"\"\"\n",
    "    Get distance between two coordinates in terms of meters.\n",
    "    \"\"\"\n",
    "    geo1 = (lat1,lon1)\n",
    "    geo2 = (lat2,lon2)\n",
    "    dist = distance.distance(geo1, geo2).m\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read relevant tables\n",
    "stop_times_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stop_times\")\n",
    "stops_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stops\")\n",
    "trips_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/trips\")\n",
    "calendar_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/calendar\")\n",
    "routes_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/routes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's filter stations in 15km radius of Zurich HB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the trips of these stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+---------------+-------------+------------------+----------------+----------------+\n",
      "|             trip_id|arrival_time|departure_time|        stop_id|stop_sequence|         stop_name|        stop_lat|        stop_lon|\n",
      "+--------------------+------------+--------------+---------------+-------------+------------------+----------------+----------------+\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:42:00|      02:42:00|    8503305:0:3|            2|        Effretikon|  47.42593043029|8.68666388288076|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:46:00|      02:46:00|    8503306:0:3|            3|         Dietlikon|47.4203145327083|8.61925430395105|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:50:00|      02:50:00|    8503147:0:1|            4|         Stettbach| 47.397334167601|8.59613166853459|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:55:00|      02:55:00|    8503003:0:1|            5|Zürich Stadelhofen|47.3667936853425|8.54846705955257|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:58:00|      03:00:00|8503000:0:41/42|            6|         Zürich HB|47.3782370338962|8.54019357578468|\n",
      "+--------------------+------------+--------------+---------------+-------------+------------------+----------------+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "stops_inradius_df= stops_df.withColumn(\"dist\",getDistToZurich(\"stop_lat\",\"stop_lon\")).filter(\"dist <= 15\")\n",
    "stops_inradius_df = stops_inradius_df.drop(\"dist\")\n",
    "#Join the stops and trips\n",
    "stops_joined_df = stop_times_df.join(stops_inradius_df,stop_times_df.stop_id == stops_inradius_df.stop_id ).drop(stops_inradius_df.stop_id)\n",
    "stops_joined_df.drop(\"parent_station\",\"location_type\",\"pickup_type\",\"drop_off_type\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write this to use it later\n",
    "#stops_joined_df.write.parquet(\"user/boecuego/stops_joined_df.parquet\")\n",
    "\n",
    "stops_joined_df = spark.read.parquet(\"user/boecuego/stops_joined_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Stop Connetions via Trips "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating all possible stop connections reachable by public transports. For example, if we have stops_ids `1,2,3,4` for `trip X`, then connections are 1-2,1-3,1-4,2-3,2-4,3-4 (in other words combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Collect all relevant information for each trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = stops_joined_df.groupBy(\"trip_id\").agg(F.collect_list(\"stop_id\").alias(\"stop_ids\"),F.collect_list(\"stop_sequence\").alias(\"stop_sequences\"),\\\n",
    "                                      F.collect_list(\"arrival_time\").alias(\"arrival_times\"),\\\n",
    "                                      F.collect_list(\"departure_time\").alias(\"departure_times\"),\\\n",
    "                                      )                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create combinations of these information \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.cache()\n",
    "df_grouped.take(1)\n",
    "df_grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs = df_grouped.withColumn(\"all_pairs\",pairs(\"stop_ids\",\"stop_sequences\",\"arrival_times\",\"departure_times\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             trip_id|            stop_ids|      stop_sequences|       arrival_times|     departure_times|           all_pairs|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591142...|[1, 2, 3, 4, 5, 6...|[28:45:00, 28:46:...|[28:45:00, 28:46:...|[[[8591315, 85911...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "df_pairs.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs_sep = df_pairs.withColumn(\"stop_id_pairs\",F.col(\"all_pairs\")[0])\\\n",
    "                        .withColumn(\"stop_sequence_pairs\",F.col(\"all_pairs\")[1])\\\n",
    "                        .withColumn(\"arrival_time_pairs\",F.col(\"all_pairs\")[2])\\\n",
    "                        .withColumn(\"departure_time_pairs\",F.col(\"all_pairs\")[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example this is how it looks for stop_id pairs, same applies for other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----+\n",
      "|             trip_id|      stop_id_pair|rown|\n",
      "+--------------------+------------------+----+\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591142]|   0|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8530811]|   1|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591104]|   2|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591429]|   3|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591180]|   4|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8530812]|   5|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591364]|   6|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8530813]|   7|\n",
      "|1.TA.26-18-j19-1.1.H|[8591142, 8530811]|   8|\n",
      "|1.TA.26-18-j19-1.1.H|[8591142, 8591104]|   9|\n",
      "+--------------------+------------------+----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df_pairs_sep.select(\"trip_id\",F.explode(\"stop_id_pairs\").alias(\"stop_id_pair\")).withColumn(\"rown\",F.monotonically_increasing_id()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sti = df_pairs_sep.select(\"trip_id\",F.explode(\"stop_id_pairs\").alias(\"stop_id_pair\")).withColumn(\"rown\",F.monotonically_increasing_id()).alias(\"sti\")\n",
    "ssp = df_pairs_sep.select(F.explode(\"stop_sequence_pairs\").alias(\"stop_sequence_pair\"),F.monotonically_increasing_id().alias(\"rown\")).alias(\"ssp\")\n",
    "atp = df_pairs_sep.select(F.explode(\"arrival_time_pairs\").alias(\"arrival_time_pair\"),F.monotonically_increasing_id().alias(\"rown\")).alias(\"atp\")\n",
    "dtp = df_pairs_sep.select(F.explode(\"departure_time_pairs\").alias(\"departure_time_pair\"),F.monotonically_increasing_id().alias(\"rown\")).alias(\"dtp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this to run faster later\n",
    "\n",
    "#ti.cache()\n",
    "#ssp.cache()\n",
    "#atp.cache()\n",
    "#dtp.cache()\n",
    "#sti.count()\n",
    "#ssp.count()\n",
    "#atp.count()\n",
    "#dtp.count()\n",
    "\n",
    "#Uncomment this to run faster later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = sti.join(ssp,sti.rown == ssp.rown).drop(ssp.rown).join(atp,sti.rown == atp.rown).drop(atp.rown).join(dtp,sti.rown == dtp.rown).drop(dtp.rown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined.write.mode(\"overwrite\").parquet(\"user/boecuego/df_all_stop_connections.parquet\")\n",
    "joined = spark.read.parquet(\"user/boecuego/df_all_stop_connections.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+------------------+--------------------+--------------------+\n",
      "|             trip_id|        stop_id_pair|rown|stop_sequence_pair|   arrival_time_pair| departure_time_pair|\n",
      "+--------------------+--------------------+----+------------------+--------------------+--------------------+\n",
      "|1.TA.26-18-j19-1.1.H|  [8591142, 8591429]|  10|            [2, 5]|[28:46:00, 28:50:00]|[28:46:00, 28:50:00]|\n",
      "|1005.TA.26-131-j1...|[8503855:0:F, 858...| 120|            [1, 7]|[16:14:00, 16:21:00]|[16:14:00, 16:21:00]|\n",
      "|1022.TA.26-33E-j1...|  [8591230, 8591196]| 423|            [6, 7]|[18:44:00, 18:46:00]|[18:44:00, 18:46:00]|\n",
      "|1022.TA.26-33E-j1...|  [8576198, 8576182]| 485|          [14, 17]|[18:55:00, 18:59:00]|[18:55:00, 18:59:00]|\n",
      "|103.TA.26-1-A-j19...|[8503512:0:2, 850...| 500|          [14, 17]|[02:51:00, 03:00:00]|[02:51:00, 03:00:00]|\n",
      "+--------------------+--------------------+----+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to filter rows to obtain trips that are available Only on Weekdays (Monday, Tuesday, Wednesday, Thursday, Friday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stop_times\")\n",
    "stops_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stops\")\n",
    "trips_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/trips\")\n",
    "calendar_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/calendar\")\n",
    "routes_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_service  =joined.join(trips_df,trips_df.trip_id ==joined.trip_id  )\n",
    "df_days = df_service.join(calendar_df,calendar_df.service_id == df_service.service_id)\n",
    "df_weekdays = df_days.filter(\"monday == 'true' AND tuesday == 'true' AND wednesday == 'true' AND thursday == 'true' AND friday == 'true' \")\n",
    "cols = joined.columns\n",
    "df_weekdays = df_weekdays.drop(trips_df.trip_id)\n",
    "joined_filtered  = df_weekdays.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined_filtered.write.mode(\"overwrite\").parquet(\"user/boecuego/df_all_stop_connections_filtered.parquet\")\n",
    "joined_filtered = spark.read.parquet(\"user/boecuego/df_all_stop_connections_filtered.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort dataframe and selet only interested columns\n",
    "joineds = joined_filtered.sort(\"rown\").select(\"trip_id\",F.col(\"stop_id_pair\").pair1.alias(\"stop_id1\")\\\n",
    "                        ,F.col(\"stop_id_pair\").pair2.alias(\"stop_id2\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair1.alias(\"stop_sequence1\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair2.alias(\"stop_sequence2\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair1.alias(\"arrival_time1\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair2.alias(\"arrival_time2\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair1.alias(\"departure_time1\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair2.alias(\"departure_time2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have all reachable stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------+--------------+-------------+-------------+---------------+---------------+\n",
      "|             trip_id|   stop_id1|stop_id2|stop_sequence1|stop_sequence2|arrival_time1|arrival_time2|departure_time1|departure_time2|\n",
      "+--------------------+-----------+--------+--------------+--------------+-------------+-------------+---------------+---------------+\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8589111|             1|             2|     16:14:00|     16:15:00|       16:14:00|       16:15:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8573553|             1|             3|     16:14:00|     16:16:00|       16:14:00|       16:16:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8573554|             1|             4|     16:14:00|     16:18:00|       16:14:00|       16:18:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8573555|             1|             5|     16:14:00|     16:19:00|       16:14:00|       16:19:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8588985|             1|             6|     16:14:00|     16:20:00|       16:14:00|       16:20:00|\n",
      "+--------------------+-----------+--------+--------------+--------------+-------------+-------------+---------------+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "joineds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Transforming the Data (Creating Connections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1- Create Stop Connections via Walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all walkable stop pair\n",
    "\n",
    "**Conditions:**\n",
    "- If distance(stop1,stop2) < 500 meters\n",
    "- If stop in radius (15km from Zurich)\n",
    "\n",
    "**Walking speed :** 50 meters in 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_inradius_df = stops_inradius_df.drop(\"dist\")\n",
    "\n",
    "stopDist = stops_inradius_df.alias(\"stops1\").crossJoin(stops_inradius_df.alias(\"stops2\"))\\\n",
    ".where(getDist(\"stops1.stop_lat\",\"stops1.stop_lon\",\"stops2.stop_lat\",\"stops2.stop_lon\")<= 500)\\\n",
    ".withColumn(\"dist\",getDist(\"stops1.stop_lat\",\"stops1.stop_lon\",\"stops2.stop_lat\",\"stops2.stop_lon\"))\n",
    "\n",
    "\n",
    "#Other option is this but don't know if it is way more efficient or more or less similiar\n",
    "#stopDistFiltered = stops_inradius_df.alias(\"stops1\").join(stops_inradius_df.alias(\"stops2\")\\\n",
    "#,getDist(\"stops1.stop_lat\",\"stops1.stop_lon\",\"stops2.stop_lat\",\"stops2.stop_lon\")<=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "walk_stops = stopDist.select(F.col(\"stops1.stop_id\").alias(\"stop1_id\")\\\n",
    "               ,F.col(\"stops1.stop_name\").alias(\"stop1_name\")\\\n",
    "               ,F.col(\"stops1.stop_lat\").alias(\"stop1_lat\")\\\n",
    "               ,F.col(\"stops1.stop_lon\").alias(\"stop1_lon\")\\\n",
    "               ,F.col(\"stops2.stop_id\").alias(\"stop2_id\")\\\n",
    "               ,F.col(\"stops2.stop_name\").alias(\"stop2_name\")\\\n",
    "               ,F.col(\"stops2.stop_lat\").alias(\"stop2_lat\")\\\n",
    "               ,F.col(\"stops2.stop_lon\").alias(\"stop2_lon\")\\\n",
    "               ,F.col(\"dist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walk_stops.write.parquet(\"user/boecuego/walk_stops.parquet\")\n",
    "\n",
    "walk_stops = spark.read.parquet(\"user/boecuego/walk_stops.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+--------------------+------------------+\n",
      "|stop1_id|          stop1_name|   stop2_id|          stop2_name|              dist|\n",
      "+--------+--------------------+-----------+--------------------+------------------+\n",
      "| 8500926|Oetwil a.d.L., Sc...|    8500926|Oetwil a.d.L., Sc...|               0.0|\n",
      "| 8500926|Oetwil a.d.L., Sc...|    8590616|Geroldswil, Schwe...|122.61607002692374|\n",
      "| 8500926|Oetwil a.d.L., Sc...|    8590737|Oetwil an der Lim...|  300.671189772556|\n",
      "| 8502186|Dietikon Stoffelbach|    8502186|Dietikon Stoffelbach|               0.0|\n",
      "| 8502186|Dietikon Stoffelbach|8502186:0:1|Dietikon Stoffelbach| 6.761029676373361|\n",
      "+--------+--------------------+-----------+--------------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#Dist is distance between two stops\n",
    "walk_stops.drop(\"stop1_lat\",\"stop1_lon\",\"stop2_lat\",\"stop2_lon\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Create Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonable_main = spark.read.parquet(\"user/boecuego/df_all_stop_connections_filtered.parquet\")\n",
    "walk_stops_main = spark.read.parquet(\"user/boecuego/walk_stops.parquet\")\n",
    "df_trip_network_reasonable = df_trip_network_reasonable_main\n",
    "walk_stops = walk_stops_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonable = df_trip_network_reasonable.sort(\"rown\").select(\"trip_id\",F.col(\"stop_id_pair\").pair1.alias(\"stop_id1\")\\\n",
    "                        ,F.col(\"stop_id_pair\").pair2.alias(\"stop_id2\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair1.alias(\"stop_sequence1\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair2.alias(\"stop_sequence2\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair1.alias(\"arrival_time1\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair2.alias(\"arrival_time2\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair1.alias(\"departure_time1\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair2.alias(\"departure_time2\"))\n",
    "joineds = df_trip_network_reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joineds = spark.read.parquet(\"user/boecuego/backtrace_df.parquet\")\n",
    "joineds.cache()\n",
    "joineds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the cost between two trip nodes:** (Arrival time of stop 2) - (Departure time of stop 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDiff = (F.unix_timestamp('arrival_time2', format=\"HH:mm:ss\")\n",
    "            - F.unix_timestamp('departure_time1', format=\"HH:mm:ss\"))\n",
    "df_trip_network_reasonable = df_trip_network_reasonable.withColumn(\"cost\",F.ceil(timeDiff/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the cost between two walking nodes:** Distance/50 + 2minutes transfer delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_stops = walk_stops.withColumn(\"cost\", F.ceil(F.col(\"dist\")/50)+2)\n",
    "walk_stops = walk_stops.select(F.col('stop1_id').alias('stop_id1'), F.col('stop2_id').alias('stop_id2'), 'cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonables =  df_trip_network_reasonable.select(\"stop_id1\",\"stop_id2\",\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonables=df_trip_network_reasonables.withColumn(\"type\",F.lit(\"trip\"))\n",
    "walk_stops = walk_stops.withColumn(\"type\",F.lit(\"walk\"))\n",
    "unioned_df = df_trip_network_reasonables.union(walk_stops)\n",
    "unioned_df = unioned_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asumption for simplicity:** If there are multiple edges between two nodes we only get the edge with minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df_unique = unioned_df.groupBy(\"stop_id1\",\"stop_id2\").agg(F.min(\"cost\").alias(\"cost\"),F.collect_set(\"type\").alias(\"types\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asumption:** We want to penalize using too many transfers so we add extra 10minutes of cost to each edge\n",
    "\n",
    "- A --> B --> C\n",
    "- A --> C\n",
    "\n",
    "we prioritize A -->C\n",
    "We are also doing this because these two routes may belong to same trip and we don't want the intermediate stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df_unique = unioned_df_unique.withColumn(\"cost\",F.col(\"cost\")+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df_unique = spark.read.parquet(\"user/boecuego/graph_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = unioned_df_unique.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph with networkx, we store cost (journey duration in minutes + penalty) and types (walking or public transport) in the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygraph = nx.from_pandas_edgelist(pd_df, 'stop_id1', 'stop_id2',[\"cost\",\"types\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Find the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the k shortest paths\n",
    "def k_shortest_paths(G, source, target, k, weight):\n",
    "    return list(islice(nx.shortest_simple_paths(G, source, target, weight=weight), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in mygraph.nodes:\n",
    "    print(node)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find **k** shortest path between two stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"8503000\" #\"8591184\"  \n",
    "tgt  = \"8591049\" #\"8591244\" \n",
    "\n",
    "src_main = src.split(\":\")[0]\n",
    "tgt_main = tgt.split(\":\")[0]\n",
    "counter = 0\n",
    "paths = []\n",
    "for node in mygraph.nodes:\n",
    "    if src_main in node:\n",
    "        counter+=1\n",
    "        \n",
    "if counter > 1:\n",
    "    k = 5\n",
    "else:\n",
    "    k=50\n",
    "for node in mygraph.nodes:\n",
    "    if src_main in node:\n",
    "        p = k_shortest_paths(mygraph, node, tgt,k,weight = \"cost\")\n",
    "        mygraph[src_main][node][\"cost\"] = 0\n",
    "        for x in p:\n",
    "            paths.append(x)\n",
    "\n",
    "\n",
    "#paths = k_shortest_paths(mygraph, src_main, tgt_main,5,weight = \"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This is a helper to get pairs in a list\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return izip(a, b)\n",
    "#When converting a date to string this is a helper\n",
    "def append_zero(x):\n",
    "    if len(str(x)) == 1:\n",
    "        return \"0\"+str(x)\n",
    "    return str(x)\n",
    "#Helper to remove adjecent walks\n",
    "def remove_adjecent_walks(paths):\n",
    "    no_adjecentwalk =[]\n",
    "    for pr in paths:\n",
    "        check = True\n",
    "        for p1,p2 in pairwise(pairwise(pr)):\n",
    "\n",
    "            if mygraph[p1[0]][p1[1]][\"types\"] == [\"walk\"] and mygraph[p2[0]][p2[1]][\"types\"] == [\"walk\"]:\n",
    "                #print(counter)\n",
    "                check = False\n",
    "\n",
    "        if check:\n",
    "            no_adjecentwalk.append(pr)\n",
    "    return no_adjecentwalk\n",
    "\n",
    "#Helper to reverse route\n",
    "def get_reverse_route(paths):\n",
    "    reversed_routes  = []\n",
    "    for route in paths:\n",
    "        lst = []\n",
    "        for a in pairwise(route):\n",
    "            lst.append(a)\n",
    "        lst_reversed = lst[::-1]\n",
    "        reversed_routes.append(lst_reversed)\n",
    "    return reversed_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want two consecutive walks, so filter them if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_adjecentwalk = remove_adjecent_walks(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know arrival time we need to **reverse** the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_routes  = get_reverse_route(no_adjecentwalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is arrival_time\n",
    "current_time = datetime.datetime(2019,1,1,12,30,0,0)\n",
    "cut_off_time = datetime.datetime(2019,1,1,11,40,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorihm:** Starting from the last connection, we try to find most suitable  trips (if not possible walks) by comparing from timetables matching the arrival time and backtrace from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notPossible = datetime.datetime(1999,1,1,12,12,0,0)\n",
    "def find_possible_routes(reversed_routes,current_timez,cut_off_time):\n",
    "    \n",
    "    shorts = []\n",
    "    longs = []\n",
    "    notpossibles = []\n",
    "    for route in reversed_routes:\n",
    "        break_loop = False\n",
    "        current_time = current_timez\n",
    "        current_trip_id = \"\"\n",
    "        \n",
    "        route_with_timetable = []\n",
    "        trip = []\n",
    "        prints  = []\n",
    "        for p1,p2 in route:\n",
    "            #p2 arrival\n",
    "            #p1 departure\n",
    "            if mygraph[p1][p2][\"types\"] == [\"walk\"]:\n",
    "                prints.append(\"Transfer with walking\")\n",
    "                prints.append((\"after_walk\",current_time))\n",
    "                cost = mygraph[p1][p2][\"cost\"]\n",
    "                if cost > 0:\n",
    "                    current_time  = current_time - datetime.timedelta(minutes=cost-10)#-2\n",
    "                else:\n",
    "                    current_time = current_time\n",
    "                prints.append((\"before_walk\",current_time))\n",
    "                current_trip_id=\"\"\n",
    "                prints.append((p1,p2))\n",
    "                continue\n",
    "            else:\n",
    "                #Get current times hour and minute\n",
    "                hour = current_time.hour\n",
    "                minute = current_time.minute\n",
    "\n",
    "                #Convert it to string\n",
    "                strtime = append_zero(hour)+\":\"+append_zero(minute)+\":00\"\n",
    "\n",
    "                #Get closest trip to current time with matching stop_ids\n",
    "                if current_trip_id != \"\":\n",
    "                    trip = joineds.filter(F.col(\"trip_id\") == current_trip_id)\\\n",
    "                    .filter(\"stop_id1 == '\"+p1+\"'\"+\" AND \"+\"stop_id2 == '\"+p2+\"'\").take(1)\n",
    "               #trip = df_trip_network_reasonable2.filter(\"prev_stop_id == '\"+p1+\"'\"+\" AND \"+\"stop_id == '\"+p2+\"'\").filter(\"arrival_time <='\"+strtime+\"'\").sort(F.desc(\"arrival_time\")).take(1)[0]\n",
    "\n",
    "                #If previous trip_id was different from this trip's id then put 2 minute waiting time\n",
    "                if trip == [] or current_trip_id == \"\":\n",
    "\n",
    "                    #2 min waiting\n",
    "                    if current_trip_id != \"\":\n",
    "                        current_time = current_time - datetime.timedelta(minutes = 2)\n",
    "                        prints.append(\"Transfer\")\n",
    "\n",
    "\n",
    "                    #Get hours, minutes\n",
    "                    hour = current_time.hour\n",
    "                    minute = current_time.minute\n",
    "                    strtime = append_zero(hour)+\":\"+append_zero(minute)+\":00\"\n",
    "\n",
    "                    #With new current time get new trip\n",
    "                    trip = joineds.filter(\"stop_id1 == '\"+p1+\"'\"+\" AND \"+\"stop_id2 == '\"+p2+\"'\").filter(\"arrival_time2 <='\"+strtime+\"'\").sort(F.desc(\"arrival_time2\")).take(1)\n",
    "                    #update current trip_id\n",
    "                try:\n",
    "                    trip = trip[0]\n",
    "                except:\n",
    "                    print(\"Encountered a route that is not possible currently\",(p1,p2))\n",
    "                    current_time = notPossible\n",
    "                    break\n",
    "                current_trip_id  = trip.trip_id\n",
    "\n",
    "                #Update current time\n",
    "                prints.append((trip.trip_id,trip.arrival_time1,trip.departure_time1,trip.arrival_time2,trip.departure_time2))\n",
    "                hms = trip.departure_time1.split(\":\")\n",
    "                h = int(hms[0])\n",
    "                m = int(hms[1])\n",
    "                current_time = datetime.datetime(2019,1,1,h,m,0,0)\n",
    "\n",
    "            prints.append((p1,p2))\n",
    "            if break_loop:\n",
    "                prints = []\n",
    "                break\n",
    "        if current_time >= cut_off_time:\n",
    "            shorts.append(prints)\n",
    "            shorts.append(current_time)\n",
    "            shorts.append(\"---------------------------------------------------------------\")\n",
    "        elif current_time == notPossible:\n",
    "            notpossibles.append(prints)\n",
    "            notpossibles.append(current_time)\n",
    "            notpossibles.append(\"---------------------------------------------------------------\")\n",
    "        else:\n",
    "            longs.append(prints)\n",
    "            longs.append(\"---------------------------------------------------------------\")\n",
    "    return shorts,longs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_routes,bad_routes = find_possible_routes(reversed_routes,current_time,cut_off_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:41/42', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:41/42', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:7', u'8580449')\n",
      "(u'235.TA.26-15-j19-1.41.H', u'11:50:00', u'11:52:00', u'11:58:00', u'11:59:00')\n",
      "(u'8503000:0:41/42', u'8503006:0:7')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'168.TA.26-12-A-j19-1.2.H', u'12:23:00', u'12:23:00', u'12:29:00', u'12:29:00')\n",
      "(u'8590620', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 23))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 19))\n",
      "(u'8503310:0:3', u'8590620')\n",
      "(u'20.TA.26-9-A-j19-1.2.H', u'12:02:00', u'12:07:00', u'12:17:00', u'12:18:00')\n",
      "(u'8503000:0:41/42', u'8503310:0:3')\n",
      "2019-01-01 12:07:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:7', u'8580449')\n",
      "(u'235.TA.26-15-j19-1.41.H', u'11:50:00', u'11:52:00', u'11:58:00', u'11:59:00')\n",
      "(u'8503000:0:41/42', u'8503006:0:7')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "(u'8503000', u'8503000:0:41/42')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'529.TA.26-12-A-j19-1.3.R', u'12:12:00', u'12:12:00', u'12:16:00', u'12:16:00')\n",
      "(u'8587655', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 12))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 9))\n",
      "(u'8503129:0:3', u'8587655')\n",
      "(u'528.TA.26-8-A-j19-1.344.H', u'11:53:00', u'11:55:00', u'12:03:00', u'12:04:00')\n",
      "(u'8503000:0:34', u'8503129:0:3')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:2', u'8580449')\n",
      "(u'528.TA.26-8-A-j19-1.344.H', u'11:53:00', u'11:55:00', u'11:59:00', u'12:00:00')\n",
      "(u'8503000:0:34', u'8503006:0:2')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 17))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 10))\n",
      "(u'8503006:0:2', u'8591382')\n",
      "(u'528.TA.26-8-A-j19-1.344.H', u'11:53:00', u'11:55:00', u'11:59:00', u'12:00:00')\n",
      "(u'8503000:0:34', u'8503006:0:2')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:5', u'8580449')\n",
      "(u'88.TA.26-36-j19-1.48.R', u'11:49:00', u'11:52:00', u'11:57:00', u'11:57:00')\n",
      "(u'8503000:0:33', u'8503006:0:5')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'529.TA.26-12-A-j19-1.3.R', u'12:12:00', u'12:12:00', u'12:16:00', u'12:16:00')\n",
      "(u'8587655', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 12))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 9))\n",
      "(u'8503129:0:3', u'8587655')\n",
      "(u'64.TA.26-19-j19-1.17.H', u'11:49:00', u'11:49:00', u'11:58:00', u'11:59:00')\n",
      "(u'8503000:0:32', u'8503129:0:3')\n",
      "2019-01-01 11:49:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:32', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:2', u'8580449')\n",
      "(u'64.TA.26-19-j19-1.17.H', u'11:49:00', u'11:49:00', u'11:54:00', u'11:55:00')\n",
      "(u'8503000:0:32', u'8503006:0:2')\n",
      "2019-01-01 11:49:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:31', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:15', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:15', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "(u'8503000:0:15', u'8591067')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:14', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:14', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:17', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:17', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:16', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:16', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:11', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:11', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:10', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:10', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:13', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:13', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:12', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:12', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:18', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:18', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000P', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000P', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 53))\n",
      "(u'8503000P', u'8591067')\n",
      "2019-01-01 11:53:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:17:00', u'12:17:00')\n",
      "(u'8587349', u'8591382')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000P', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'46.TA.26-79-j19-1.1.R', u'12:18:00', u'12:18:00', u'12:22:00', u'12:22:00')\n",
      "(u'8591349', '8591049')\n",
      "Transfer\n",
      "(u'1183.TA.26-7-B-j19-1.6.R', u'11:59:00', u'11:59:00', u'12:12:00', u'12:12:00')\n",
      "(u'8591174', u'8591349')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 49))\n",
      "(u'8503000P', u'8591174')\n",
      "2019-01-01 11:49:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:9', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:9', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:8', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:8', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 51))\n",
      "(u'8503000:0:8', u'8591067')\n",
      "2019-01-01 11:51:00\n",
      "---------------------------------------------------------------\n",
      "(u'46.TA.26-79-j19-1.1.R', u'12:18:00', u'12:18:00', u'12:22:00', u'12:22:00')\n",
      "(u'8591349', '8591049')\n",
      "Transfer\n",
      "(u'1183.TA.26-7-B-j19-1.6.R', u'11:59:00', u'11:59:00', u'12:12:00', u'12:12:00')\n",
      "(u'8591174', u'8591349')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 50))\n",
      "(u'8503000:0:8', u'8591174')\n",
      "2019-01-01 11:50:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:17:00', u'12:17:00')\n",
      "(u'8587349', u'8591382')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:8', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:3', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:3', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:6', u'8580449')\n",
      "(u'204.TA.26-24-j19-1.121.R', u'11:39:00', u'11:44:00', u'11:51:00', u'11:52:00')\n",
      "(u'8503000:0:3', u'8503006:0:6')\n",
      "2019-01-01 11:44:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:5', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:5', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:6', u'8580449')\n",
      "(u'32.TA.80-159-Y-j19-1.8.H', u'12:05:00', u'12:05:00', u'12:11:00', u'12:11:00')\n",
      "(u'8503000:0:5', u'8503006:0:6')\n",
      "2019-01-01 12:05:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:4', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:4', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:7', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:7', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "(u'8503000:0:7', u'8591067')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'46.TA.26-79-j19-1.1.R', u'12:18:00', u'12:18:00', u'12:22:00', u'12:22:00')\n",
      "(u'8591349', '8591049')\n",
      "Transfer\n",
      "(u'1183.TA.26-7-B-j19-1.6.R', u'11:59:00', u'11:59:00', u'12:12:00', u'12:12:00')\n",
      "(u'8591174', u'8591349')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 50))\n",
      "(u'8503000:0:7', u'8591174')\n",
      "2019-01-01 11:50:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:17:00', u'12:17:00')\n",
      "(u'8587349', u'8591382')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:7', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:6', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:6', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:43/44', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:43/44', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "for element in good_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in bad_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best route(s)\n",
    "\n",
    "**Assumption:** Best route(s) is(are) the route(s) with latest departure(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxt = datetime.datetime(2019, 1, 1, 0,0)\n",
    "\n",
    "index = -1\n",
    "start_times  = []\n",
    "for i,t in enumerate(good_routes):\n",
    "    if type(t) == type(datetime.datetime(2019, 1, 1, 11, 56)):\n",
    "        start_times.append((t,i))\n",
    "        if t > maxt:\n",
    "            maxt = t\n",
    "            index = i\n",
    "start_times_reversed = sorted(start_times, key=lambda tup: tup[0],reverse=True)\n",
    "best_routes = []\n",
    "for time,index in start_times_reversed[:10]:\n",
    "    route = good_routes[index-1:index]\n",
    "    best_routes.append(route[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print best route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we did backtracing to find the best routes, we should read this data from end to start. For example, the passanger first takes the trip ID `'629.TA.26-32-j19-1.10.R'` in order to go from the station ID `8591184` to station ID `8591101`. The first time entry is the arrival time to the previous stop (in this case the stop with the ID `8591184`), the second time entry is the departure time from the previous stop. The third time entry is the arrival time to the current stop (in this case the stop with the ID `8591101`), the fourth time entry is the departure time from the current stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "(u'168.TA.26-12-A-j19-1.2.H', u'12:23:00', u'12:23:00', u'12:29:00', u'12:29:00')\n",
      "(u'8590620', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 23))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 19))\n",
      "(u'8503310:0:3', u'8590620')\n",
      "(u'20.TA.26-9-A-j19-1.2.H', u'12:02:00', u'12:07:00', u'12:17:00', u'12:18:00')\n",
      "(u'8503000:0:41/42', u'8503310:0:3')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:6', u'8580449')\n",
      "(u'32.TA.80-159-Y-j19-1.8.H', u'12:05:00', u'12:05:00', u'12:11:00', u'12:11:00')\n",
      "(u'8503000:0:5', u'8503006:0:6')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:41/42', u'8587349')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000', u'8587349')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:32', u'8591379')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:31', u'8591379')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:14', u'8587349')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:17', u'8591379')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:11', u'8591379')\n",
      "************************************************************\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:10', u'8591379')"
     ]
    }
   ],
   "source": [
    "for r in best_routes:\n",
    "    print(\"******\"*10)\n",
    "    for x in r:\n",
    "        print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_routes = good_routes[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stops\")\n",
    "stops_inradius_df= stops_df.withColumn(\"dist\",getDistToZurich(\"stop_lat\",\"stop_lon\")).filter(\"dist <= 15\")\n",
    "\n",
    "stops_inradius2 = set(stops_inradius_df.select(\"stop_id\").distinct().rdd.map(lambda r: r[0]).collect())\n",
    "stops_inradius = stops_inradius2.copy()\n",
    "for s in stops_inradius2:\n",
    "    if \":\" in s:\n",
    "        stops_inradius.add(s.split(\":\")[0])\n",
    "sbb = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "sbb_inradius = sbb.where(F.col(\"bpuic\").isin(stops_inradius))\n",
    "\n",
    "sbb_inradius_weekdays = sbb_inradius.withColumn('date', F.from_unixtime(F.unix_timestamp('betriebstag', 'dd.MM.yyy')))\\\n",
    "    .withColumn(\"day\",F.dayofweek(\"date\"))\\\n",
    "        .filter(\"day BETWEEN 2 AND 6\")\n",
    "\n",
    "\n",
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'LINIEN_TEXT':'line',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "sbb_inradius_weekdays_english = sbb_inradius_weekdays.selectExpr([k + ' as ' + fields[k] for k in fields])\n",
    "\n",
    "sbb_inradius_weekdays_english = sbb_inradius_weekdays_english.withColumn(\"arrival_time_schedule\",\\\n",
    "                                                  sbb_inradius_weekdays_english.schedule_arrival.substr(12,10000))\\\n",
    "                                                    .withColumn(\"departure_time_schedule\",\\\n",
    "                                                  sbb_inradius_weekdays_english.schedule_dep.substr(12,10000))\\\n",
    "                                                    .withColumn(\"arrival_time_real\",\\\n",
    "                                                  sbb_inradius_weekdays_english.real_arrival.substr(12,10000))\\\n",
    "                                                    .withColumn(\"departure_time_real\",\\\n",
    "                                                  sbb_inradius_weekdays_english.real_dep.substr(12,10000))\n",
    "#sbb_inradius_weekdays_english.write.parquet(\"user/boecuego/sbb_inradius_weekdays_english.parquet\")\n",
    "\n",
    "\n",
    "sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_weekdays_english.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only stop id's also exist between 13-17 May 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the trip_ids available only between 13-17 May\n",
    "\n",
    "\n",
    "sbb_inradius_13_17_trips = sbb_inradius.filter(\"date <= '17.09.2019' AND date >= '13.09.2019' \").select(\"trip_id\").distinct()\n",
    "sbb_inradius_13_17_trips_select = sbb_inradius.join(F.broadcast(sbb_inradius_13_17_trips), sbb_inradius.trip_id == sbb_inradius_13_17_trips.trip_id)\n",
    "sbb_inradius_13_17_trips_select = sbb_inradius_13_17_trips_select.drop(sbb_inradius.trip_id)\n",
    "#sbb_inradius_13_17_trips_select.write.parquet(\"user/boecuego/sbb_inradius_13_17_trips_select2.parquet\")\n",
    "\n",
    "sbb_inradius_13_17_trips_select = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_select2.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect also stop_ids of each trip \n",
    "\n",
    "For example -> trip_id1 : [stopid1,stopid2,...stopid7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_trip_stop_ids = sbb_inradius_13_17_trips_select.groupBy(\"trip_id\",\"date\")\\\n",
    ".agg(F.collect_list(\"stop_id\").alias(\"stop_ids\"))\\\n",
    ".groupBy(\"trip_id\").agg(F.first(\"stop_ids\").alias(\"stop_ids\"))\n",
    "#sbb_trip_stop_ids.write.parquet(\"user/boecuego/sbb_trip_stop_ids2.parquet\")\n",
    "sbb_trip_stop_ids = spark.read.parquet(\"user/boecuego/sbb_trip_stop_ids2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_inradius_13_17_trips_wstops = sbb_inradius_13_17_trips_select\\\n",
    ".join(F.broadcast(sbb_trip_stop_ids), sbb_inradius_13_17_trips_select.trip_id == sbb_trip_stop_ids.trip_id)\n",
    "sbb_inradius_13_17_trips_wstops = sbb_inradius_13_17_trips_wstops.drop(sbb_inradius_13_17_trips_select.trip_id)\n",
    "#sbb_inradius_13_17_trips_wstops.write.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\")\n",
    "\n",
    "sbb_inradius_13_17_trips_wstops = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sbb_inradius.write.parquet(\"user/boecuego/sbb_inradius_weekdays_engl.parquet\")\n",
    "#####\n",
    "#sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_weekdays_engl.parquet\") #this is weekdays\n",
    "###\n",
    "#sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_select.parquet\") #this is only trip_ids from 13_17\n",
    "sbb_inradius_wostops = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\")\n",
    "sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\") #Also added stop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sbb_grouped_delay = sbb_inradius_wostops.withColumn(\"delay\",F.floor(timeDiff/60)).groupBy(\"stop_id\",\"delay\").count()\n",
    "# sbb_grouped_delay.write.parquet(\"user/boecuego/sbb_stops_time_grouped.parquet\")\n",
    "\n",
    "sbb_grouped_delay = spark.read.parquet(\"user/boecuego/sbb_stops_time_grouped.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df.filter(\"stop_name LIKE '%Langmauerstrasse%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! TO DO !!!!!\n",
    "#Filter weekends, public holidays etc\n",
    "#!!! TO DO !!!!!\n",
    "def find_prob(stop_id,arrival_time_schedule):\n",
    "    timeDiff = (F.unix_timestamp('arrival_time_real', format=\"HH:mm:ss\")\n",
    "            - F.unix_timestamp('arrival_time_schedule', format=\"HH:mm\"))\n",
    "    delays = sbb_grouped_delay.filter((F.col('stop_id') == stop_id) & time_interval_udf(F.col('arrival_time_schedule'), F.lit(arrival_time_schedule)))\n",
    "    #Find the total num\n",
    "    #total_num = delays.groupBy().sum(\"count\").collect()[0][0]\n",
    "    #print(total_num)\n",
    "    delays_pd = delays.toPandas()\n",
    "    total_num = sum(delays_pd[\"count\"].values)\n",
    "    delays_pd[\"percentage\"] = delays_pd[\"count\"].map(lambda x:(x/float(total_num))*100)\n",
    "    return delays_pd\n",
    "    \n",
    "\"\"\"\n",
    "def find_prob_wprevstop(stop_id,arrival_time_schedule,prev_stop_id):\n",
    "    timeDiff = (F.unix_timestamp('arrival_time_real', format=\"HH:mm:ss\")\n",
    "            - F.unix_timestamp('arrival_time_schedule', format=\"HH:mm\"))\n",
    "    delays = sbb_inradius.filter((F.col(\"stop_id\") == stop_id) \\\n",
    "    & (time_interval_udf(\"arrival_time_schedule\",F.lit(arrival_time_schedule))) \\\n",
    "    & (F.array_contains(sbb_inradius.stop_ids,prev_stop_id ))).withColumn(\"delay\",F.floor(timeDiff/60)).groupBy(\"delay\").count()\n",
    "    #Find the total num\n",
    "    #total_num = delays.groupBy().sum(\"count\").collect()[0][0]\n",
    "    #print(total_num)\n",
    "    delays_pd = delays.toPandas()\n",
    "    total_num = sum(delays_pd[\"count\"].values)\n",
    "    delays_pd[\"percentage\"] = delays_pd[\"count\"].map(lambda x:(x/float(total_num))*100)\n",
    "    return delays_pd    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find probabiliy for the good routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each transfer, we calculate the probability of not missing the transfer vehicle. Assume that a passanger does $n$ transfers for the planned journey. Then, the probability of arriving in time is:\n",
    "\n",
    "\\begin{align}\n",
    "Pr[\\textrm{Arriving in time with the found route}] & = Pr[\\textrm{Not missing any transfers}] \\cdot Pr[\\textrm{Arriving the final station in time}] \\\\\n",
    "                                                   & = \\prod_{i=0}^{n-1} Pr[\\textrm{Not missing transfer i}] \\cdot Pr[\\textrm{Arriving the final station in time}]\n",
    "\\end{align}\n",
    "\n",
    "where the second equality follows from the assumption that **the delays of transports are independent of each other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### !!!!!!! TO CHECK !!!!!!!!\n",
    "# This is currently not checking the probabiliy of desired arival time Also need to check that\n",
    "#!!!!!!! TO CHECK !!!!!!!!\n",
    "confidence_level = 0.98\n",
    "def calculate_probabilities(desired_arrival_time,best_routes, best_routes_print,confidence_level):\n",
    "    counter = 0\n",
    "    for r,printroute in zip(best_routes,best_routes_print):\n",
    "        overall_prob = 1.\n",
    "        \n",
    "        printornot = []\n",
    "        for i in range(len(r)):\n",
    "            # This case corresponds to the Pr[Arriving the final station in time] case\n",
    "            # That is why the index is zero and the final means of travelling should not be walking but a public transport\n",
    "            if i == 0 and r[i]!=\"Transfer with walking\":\n",
    "                las_arrival_time = r[i][3]\n",
    "                ah = las_arrival_time.split(\":\")[0]\n",
    "                am = las_arrival_time.split(\":\")[1]\n",
    "                a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "                last_station = r[i+1][1]\n",
    "                \n",
    "                allowed_delay = (desired_arrival_time-a).total_seconds()/60\n",
    "                prob_dist =find_prob(last_station,las_arrival_time[:-3])\n",
    "                probability_to_catch = sum(prob_dist[prob_dist[\"delay\"]<= allowed_delay].percentage.values)\n",
    "                overall_prob = (float(overall_prob)*float(probability_to_catch))/100.\n",
    "                printornot.append(\"Prob to arrive on time: \"+str(probability_to_catch))\n",
    "\n",
    "            try:\n",
    "                if r[i] == \"Transfer with walking\":\n",
    "\n",
    "                    transfer_departure_time = r[i-2][2]\n",
    "                    dh = transfer_departure_time.split(\":\")[0]\n",
    "                    dm = transfer_departure_time.split(\":\")[1]\n",
    "                    d = datetime.datetime(2019, 1, 1, int(dh), int(dm))\n",
    "                    #think this\n",
    "                    transfer_station =  r[i+5][1].split(\":\")[0]\n",
    "\n",
    "                    walking_min = (r[i+1][1] - r[i+2][1]).total_seconds()/60\n",
    "\n",
    "                    #think this\n",
    "                    transfer_arrival_time = r[i+4][3]\n",
    "            \n",
    "\n",
    "                    ah = transfer_arrival_time.split(\":\")[0]\n",
    "                    am = transfer_arrival_time.split(\":\")[1]\n",
    "                    a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "                    allowed_delay = (d-a).total_seconds()/60 - walking_min\n",
    "                    #print(transfer_station,transfer_arrival_time)\n",
    "                    prob_dist =find_prob(transfer_station,transfer_arrival_time[:-3])\n",
    "\n",
    "                    probability_to_catch = sum(prob_dist[prob_dist[\"delay\"]<= allowed_delay].percentage.values)\n",
    "                    \n",
    "                    printornot.append(\"Probability to catch trip \"+r[i-2][0]+\" with \"+str(walking_min)+\" min walking  : \"+str(probability_to_catch))\n",
    "                    \n",
    "                    overall_prob = (float(overall_prob)*float(probability_to_catch))/100.\n",
    "                \n",
    "                elif r[i] == \"Transfer\":\n",
    "                    transfer_departure_time = r[i-2][2]\n",
    "                    dh = transfer_departure_time.split(\":\")[0]\n",
    "                    dm = transfer_departure_time.split(\":\")[1]\n",
    "                    d = datetime.datetime(2019, 1, 1, int(dh), int(dm))\n",
    "                    transfer_station =  r[i-1][1]\n",
    "                    \n",
    "                    transfer_arrival_time = r[i+1][3]\n",
    "                    \n",
    "                    ah = transfer_arrival_time.split(\":\")[0]\n",
    "                    am = transfer_arrival_time.split(\":\")[1]\n",
    "                    a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "                    allowed_delay = (d-a).total_seconds()/60\n",
    "                    prob_dist =find_prob(transfer_station,transfer_arrival_time[:-3])\n",
    "                    \n",
    "                    probability_to_catch = sum(prob_dist[prob_dist[\"delay\"]<= allowed_delay].percentage.values)\n",
    "                    printornot.append(\"Probability to catch trip \"+r[i-2][0]+\" : \"+str(probability_to_catch))\n",
    "                    overall_prob = (float(overall_prob)*float(probability_to_catch))/100.\n",
    "\n",
    "            except:\n",
    "                print(\"-\")\n",
    "        \n",
    "        if overall_prob > confidence_level:\n",
    "            counter +=1\n",
    "            print(\"*\"*100)\n",
    "            print(\"Overal probability: \"+str(overall_prob))\n",
    "            print(\"Probabilities: \")\n",
    "            for prt in printornot:\n",
    "                print \"\\t\"+prt\n",
    "            print(\"Route: \")\n",
    "            for rou in printroute:\n",
    "                print \"\\t\"+rou\n",
    "            #print(\"Probability matches the confidence level, we can select this route.\")\n",
    "        else:\n",
    "            warning = 1\n",
    "            #for rou in printroute:\n",
    "            #    print rou\n",
    "            #print(\"WARNING: Confidence level is lower than the required amount! Do not select this route!\")\n",
    "    print str(counter)+\" routes found out of \"+str(len(best_routes))+\" best routes , if you want more please enter lower confidence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "calculate_probabilities() takes exactly 4 arguments (3 given)\n",
      "Traceback (most recent call last):\n",
      "TypeError: calculate_probabilities() takes exactly 4 arguments (3 given)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_probabilities(current_time,best_routes, confidence_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def schedule_planner(src, tgt, desired_arrival_time, confidence_level):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    confidence_level=float(confidence_level)\n",
    "    \n",
    "    src_main = src.split(\":\")[0]\n",
    "    tgt_main = tgt.split(\":\")[0]\n",
    "    counter = 0\n",
    "    paths = []\n",
    "    for node in mygraph.nodes:\n",
    "        if src_main in node:\n",
    "            counter+=1\n",
    "\n",
    "    if counter > 1:\n",
    "        k = 5\n",
    "    else:\n",
    "        k=50\n",
    "    for node in mygraph.nodes:\n",
    "        if src_main in node:\n",
    "            p = k_shortest_paths(mygraph, node, tgt,k,weight = \"cost\")\n",
    "            for x in p:\n",
    "                paths.append(x)\n",
    "    no_adjecentwalk = remove_adjecent_walks(paths)\n",
    "    reversed_routes  = get_reverse_route(no_adjecentwalk)\n",
    "    \n",
    "    hm = desired_arrival_time.split(':')\n",
    "    h = hm[0]\n",
    "    m = hm[1]\n",
    "    #This is arrival_time\n",
    "    current_time = datetime.datetime(2019,1,1,int(h),int(m),0,0)\n",
    "    cut_off_time = current_time - datetime.timedelta(minutes=60)\n",
    "    \n",
    "    notPossible = datetime.datetime(1999,1,1,12,12,0,0)\n",
    "    good_routes,bad_routes,printouts = find_possible_routes(reversed_routes,current_time,cut_off_time)\n",
    "    \n",
    "    \"\"\"\n",
    "    for element in good_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element\n",
    "            \n",
    "    for element in bad_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element\n",
    "    \"\"\"\n",
    "            \n",
    "    maxt = datetime.datetime(2019, 1, 1, 0,0)\n",
    "    index = -1\n",
    "    start_times  = []\n",
    "    for i,t in enumerate(good_routes):\n",
    "        if type(t) == type(datetime.datetime(2019, 1, 1, 11, 56)):\n",
    "            start_times.append((t,i))\n",
    "            if t > maxt:\n",
    "                maxt = t\n",
    "                index = i\n",
    "    start_times_reversed = sorted(start_times, key=lambda tup: tup[0],reverse=True)\n",
    "    best_routes = []\n",
    "    best_routes_print = [] \n",
    "\n",
    "    for time,index in start_times_reversed[:10]:\n",
    "        route = good_routes[index-1:index]\n",
    "        best_routes.append(route[0])\n",
    "        route_print = printouts[index/3]\n",
    "        best_routes_print.append(route_print)\n",
    "    \"\"\"\n",
    "    for r in best_routes_print:\n",
    "        print(\"******\"*10)\n",
    "        for x in r:\n",
    "            print x\n",
    "    for r in best_routes:\n",
    "        print(\"******\"*10)\n",
    "        for x in r:\n",
    "            print x\"\"\"\n",
    "    \n",
    "    print('*'*100)\n",
    "    print('PROBABILITIES')\n",
    "            \n",
    "    only_routes = good_routes[::3]\n",
    "    calculate_probabilities(current_time,best_routes,best_routes_print, confidence_level)\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print('*'*100)\n",
    "    print(\"Running time:\", (end-start).total_seconds(), \"seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ffa005f89946f587c8225f5c523463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='8503000', description='src'), Text(value='8591049', description='tgt'), Text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.troll(src, tgt, desired_arrival_time, confidence_level)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "def troll(src, tgt, desired_arrival_time, confidence_level):\n",
    "    # To push it to the spark\n",
    "    confidence_level = str(confidence_level)\n",
    "    \n",
    "    get_ipython().push(\"src\")\n",
    "    get_ipython().push(\"tgt\")\n",
    "    get_ipython().push(\"desired_arrival_time\")\n",
    "    get_ipython().push(\"confidence_level\")\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i src -t str -n src' , ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i tgt -t str -n tgt' , ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i desired_arrival_time -t str -n desired_arrival_time' , ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i confidence_level -t str -n confidence_level' , ' ')\n",
    "\n",
    "    \n",
    "    get_ipython().run_cell_magic('spark', '', 'schedule_planner(src, tgt, desired_arrival_time, confidence_level)') \n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets \n",
    "func_handle = interact_manual(troll, src='8503000', tgt='8591049', desired_arrival_time='12:30',\n",
    "                                confidence_level=widgets.FloatSlider(min=0.0, max=1.0, step=1e-2, value=0.90))\n",
    "func_handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate the validity of this solution in one month (May 2019):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "matched_trips = spark.read.parquet(\"user/boecuego/matched_trips2.parquet\") \n",
    "mmm = matched_trips.filter(\"LENGTH(trips_stop_ids) > 15 AND sample_stop_ids != '%%' \").groupBy(\"trips_trip_id\").agg(F.collect_set(\"sbb_trip_id\").alias(\"possible_match\"))\n",
    "stop_matches = map(lambda row: row.asDict(), mmm.collect())\n",
    "dict_matches = {match['trips_trip_id']: match[\"possible_match\"] for match in stop_matches}\n",
    "sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_weekdays_english.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "Validation from the real past data:\n",
      "1.0\n",
      "**************************************************\n",
      "Time to run the validation code in seconds:\n",
      "1485.25756"
     ]
    }
   ],
   "source": [
    "t1=datetime.datetime.now()\n",
    "#Trips are from above result \n",
    "trips = [\"168.TA.26-12-A-j19-1.2.H\",\"20.TA.26-9-A-j19-1.2.H\"]\n",
    "departs = [\"12:23:00\",\"12:07:00\"]\n",
    "arrives = [\"12:29:00\",\"12:17:00\"]\n",
    "allz = zip(pairwise(trips),pairwise(departs),pairwise(arrives))\n",
    "dict_matches[\"32.TA.80-159-Y-j19-1.8.H\"] = [\"85:11:18844:001\"]\n",
    "\n",
    "days = [append_zero(str(x))+\".05.2019\" for x in range(1,30)]\n",
    "weekdayslst = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\"]\n",
    "\n",
    "weekdays = [x if datetime.datetime.strptime(x, '%d.%m.%Y').strftime('%A') in weekdayslst else \"\" for x in days]\n",
    "weekdays= filter(None,weekdays)\n",
    "\n",
    "catched_counter = 0\n",
    "counter = 0\n",
    "for day in weekdays:\n",
    "    print(\"*\"*50)\n",
    "    sbd = sbb_inradius.filter(F.col(\"date\")==day)\n",
    "    for trip,depart,arrive in reversed(allz):\n",
    "        print(\"- -\"*30)\n",
    "        ilkduraktankalkan = dict_matches[trip[1]]\n",
    "        ikinciduraktankalkan = dict_matches[trip[0]]\n",
    "        try:\n",
    "            ilkininin_ikinciye_varisi = sbd.filter((F.col(\"trip_id\").isin(ilkduraktankalkan)) \\\n",
    "                                                            &(F.substring(F.col(\"schedule_arrival\"),12,100) == arrive[1][:-3])).take(1)[0]\n",
    "            ikincinin_kalkisi = sbd.filter((F.col(\"trip_id\").isin(ikinciduraktankalkan)) \\\n",
    "                                                            &(F.substring(F.col(\"schedule_dep\"),12,100) == depart[0][:-3])).take(1)[0]\n",
    "\n",
    "            hm = ilkininin_ikinciye_varisi.arrival_time_real.split(':')\n",
    "            h = hm[0]\n",
    "            m = hm[1]\n",
    "            s = hm[2]\n",
    "            aa = datetime.datetime(2019,1,1,int(h),int(m),int(s),0)\n",
    "\n",
    "            hm = ikincinin_kalkisi.departure_time_real.split(':')\n",
    "            h = hm[0]\n",
    "            m = hm[1]\n",
    "            s = hm[2]\n",
    "            ad = datetime.datetime(2019,1,1,int(h),int(m),int(s),0)\n",
    "\n",
    "            window = (ad-aa).total_seconds()\n",
    "            #print(window/60)\n",
    "            counter +=1\n",
    "            if window >=0:\n",
    "                catched_counter+=1\n",
    "\n",
    "\n",
    "        except:\n",
    "            print \"Except\"\n",
    "t2=datetime.datetime.now()\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "print(\"Validation from the real past data:\")\n",
    "print(float(catched_counter)/float(counter))\n",
    "print(\"*\"*50)\n",
    "print(\"Time to run the validation code in seconds:\")\n",
    "print((t2-t1).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

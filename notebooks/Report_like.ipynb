{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Journey Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Transforming Data (Connecting Stops etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing complex operations like finding shortest paths, calculating arrive on time probabilities etc. We need to transform the data to proper format. Unde that section we performed joins, groupbys, adding new columns, ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create Stop Connections for Trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, by using data from timetables, we created a connectivity matrix like dataframe. In that dataframe, we have all possible connections. Below you can see an illustration of an exampe connection. Blue circles belong to same trip.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditions:**\n",
    "- If stop in radius (15km from Zurich)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"connections.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Stop Connections for Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we connected walkable stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditions:**\n",
    "- If distance(stop1,stop2) < 500 meters\n",
    "- If stop in radius (15km from Zurich)\n",
    "\n",
    "**Walking speed :** 50 meters in 1 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Match Trip IDs of SBB and Timetable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SBB data and Timetable data have different trip ids, we wanted to match those trip ids. However, there were very few exact matched.(Exact same stop ids with exact same departure times). So instead we came up with a different solution.\n",
    "\n",
    "Let's assume we have two trips from sbb and timetable and they have respected stops :\n",
    "\n",
    "sbb : 1-2-3-4-5-6-7-8\n",
    "timetable : 2-3-4-5-6-7\n",
    "\n",
    "We noticed that this is the case for most of the trips, very similiar stop ids but not exactly same\n",
    "\n",
    "So we used a partially match, which means if most of the stop ids match and departure times are similiar (e.g. 19:17 and 19:15) we matched those trip ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Creating Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the graph, first we merged two dataframes (walking connections and trips connections).\n",
    "\n",
    "**Calculate the cost between two trip nodes:** (Arrival time of stop 2) - (Departure time of stop 1)\n",
    "\n",
    "**Calculate the cost between two walking nodes:** Distance/50 + 2minutes transfer delay\n",
    "\n",
    "**Select edge with minimum cost:** If there are multiple connections between two nodes we only select the edge with the minimum cost\n",
    "\n",
    "**Penalize:** We want to penalize using too many transfers so we add extra 10minutes of cost to each edge, also this way we prevent having multiple same routes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graph.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Finding the Short Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1- Finding initial shorting paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we found top **k** shortest paths by using a function derives from Dijkstra's shortest path algorithm. For that part if the stop id is a main station with multiple platforms then we calculate multiple paths from each platform and combine them to find in total **k** paths. Otherwise we directly generate **k** shortest paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2- Fitting shortest paths to time table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since out initial short paths are based only on duration between stops, it does not take waiting times into account. In other words it is possible that some of our initial paths are not available for the desired time, **OR** it takes longer then expected due to waiting/transfer times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backtracing:** So in this step we backtrace from desired arrival time and try to fit our shortest paths to the timetable. Then we select top 10 routes, according to their departure time (greater the departure time better the route)\n",
    "\n",
    "For example, let's say our shortest paths last two stops are **stn-1** and **stn**, and desired arrival time is **12:30** pm. We try to find a trip between **stn-1** and **stn** where arrival time to **stn** is less or equal to 12:30. If we find a trip, then we go to **stn-2** and **stn-1** stop pairs(previous pair) apply similiar logic, but now or desired arrival time is departure time of stn-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Finding Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate probabilities we are using stop id delays. For each stop id and arrival time pair we calculated distribution of delays(histogram) and by using that we calculated probability of each delay for a givien stop and time. We are using all the data from sbb dataframe where date is a weekday.\n",
    "\n",
    "Then for the top 10 routes we obtained, we calculate probability to catch a transfer(trip1 to trip2) and arrive on time. \n",
    "\n",
    "\n",
    "**Conditions for transfer:** \n",
    "- If there is walking between two stops then maxiumum allowed delay =  (departure of trip2 - arrival of trip1) - walking duration\n",
    "- If there is **NO** walking between two stops then maxiumum allowed delay =  (departure of trip2 - arrival of trip1) - 2 min\n",
    "\n",
    "**Conditions for arrival:** \n",
    "- allowed delay = (desired arrival time - arrival of last trip)\n",
    "\n",
    "\n",
    "For example if first trip arrival time is 12:30 and second trip's departure time is 12:45 and there is a walking duration of 6 minutes between those stops then allowed delay is 9 minutes and catching probability is **P**(delay<=9min)\n",
    "\n",
    "\n",
    "\n",
    "**Note** We could use trip ids, however we were not able to match all trip ids between sbb data and timetable data. So we went with more generalized solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation we are checking all the weekdays of May 2019 to see if we would catch the transfer/arrive on time. So basically for 20 days if we arrive on time on 18 days and miss the transfer on 2 days then it is -> 18/20 ~ 90% And then compare with our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\":\"oh-my-git_final\",\n",
    "     \"driverMemory\": \"2000M\",\n",
    "    \"executorMemory\": \"8G\",\n",
    "    \"executorCores\": 8,\n",
    "    \"numExecutors\": 16\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\":\"oh-my-git_final\",\n",
    "     \"driverMemory\": \"2000M\",\n",
    "    \"executorMemory\": \"8G\",\n",
    "    \"executorCores\": 4,\n",
    "    \"numExecutors\": 20\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data and Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from geopy import distance\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions  as F\n",
    "from pyspark.sql.window import Window\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from itertools import islice, tee, izip, combinations\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Some constants\n",
    "zurichHB = (47.378177, 8.540192)\n",
    "cleveland_oh = (49.378177, 9.540192)\n",
    "print(distance.distance(zurichHB, cleveland_oh).m)\n",
    "\n",
    "# Useful user-defined functions\n",
    "@F.udf(T.DoubleType())\n",
    "def getDistToZurich(lat,lon):\n",
    "    \"\"\"\n",
    "    Get distance to the Zurich HB in kilometers.\n",
    "    \"\"\"\n",
    "    zurichHB = (47.378177, 8.540192)\n",
    "    geo = (lat,lon)\n",
    "    return distance.distance(zurichHB, geo).km\n",
    "\n",
    "@F.udf(T.ArrayType(\n",
    "       T.ArrayType(T.StructType([\n",
    "    T.StructField(\"pair1\", T.StringType(), False),\n",
    "    T.StructField(\"pair2\", T.StringType(), False)\n",
    "]))))\n",
    "def pairs(stop_ids,stop_sequences,arrival_times,departure_times):\n",
    "    \"\"\"\n",
    "    Print the journey information in pairs.\n",
    "    \"\"\"\n",
    "    stop_id_pairs = list(combinations(stop_ids,2))\n",
    "    stop_sequence_pairs = list(combinations(stop_sequences,2))\n",
    "    arrival_time_pairs = list(combinations(arrival_times,2))\n",
    "    departure_time_pairs = list(combinations(departure_times,2))\n",
    "    return [stop_id_pairs,stop_sequence_pairs,arrival_time_pairs,departure_time_pairs]\n",
    "\n",
    "@F.udf(T.BooleanType())\n",
    "def time_interval_udf(arrival_time_schedule,transfer_arrival_time):\n",
    "    \"\"\"\n",
    "    Get a 2 minute time interval in order to do find relevant vehicles in the SBB data.\n",
    "    \"\"\"\n",
    "    if arrival_time_schedule == \"\":\n",
    "        return False\n",
    "    ah = transfer_arrival_time.split(\":\")[0]\n",
    "    am = transfer_arrival_time.split(\":\")[1]\n",
    "    a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "    \n",
    "    atsh = arrival_time_schedule.split(\":\")[0]\n",
    "    atsm = arrival_time_schedule.split(\":\")[1]\n",
    "    ats = datetime.datetime(2019, 1, 1, int(atsh), int(atsm))\n",
    "    \n",
    "    #Allow 120 seconds time difference\n",
    "    if abs(ats-a).total_seconds() <= 120:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "@F.udf(T.DoubleType())\n",
    "def getDist(lat1,lon1,lat2,lon2):\n",
    "    \"\"\"\n",
    "    Get distance between two coordinates in terms of meters.\n",
    "    \"\"\"\n",
    "    geo1 = (lat1,lon1)\n",
    "    geo2 = (lat2,lon2)\n",
    "    dist = distance.distance(geo1, geo2).m\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read relevant tables\n",
    "stop_times_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stop_times\")\n",
    "stops_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stops\")\n",
    "trips_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/trips\")\n",
    "calendar_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/calendar\")\n",
    "routes_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/routes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's filter stations in 15km radius of Zurich HB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the trips of these stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+---------------+-------------+------------------+----------------+----------------+\n",
      "|             trip_id|arrival_time|departure_time|        stop_id|stop_sequence|         stop_name|        stop_lat|        stop_lon|\n",
      "+--------------------+------------+--------------+---------------+-------------+------------------+----------------+----------------+\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:42:00|      02:42:00|    8503305:0:3|            2|        Effretikon|  47.42593043029|8.68666388288076|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:46:00|      02:46:00|    8503306:0:3|            3|         Dietlikon|47.4203145327083|8.61925430395105|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:50:00|      02:50:00|    8503147:0:1|            4|         Stettbach| 47.397334167601|8.59613166853459|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:55:00|      02:55:00|    8503003:0:1|            5|Zürich Stadelhofen|47.3667936853425|8.54846705955257|\n",
      "|5.TA.1-1-A-j19-1.3.H|    02:58:00|      03:00:00|8503000:0:41/42|            6|         Zürich HB|47.3782370338962|8.54019357578468|\n",
      "+--------------------+------------+--------------+---------------+-------------+------------------+----------------+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "stops_inradius_df= stops_df.withColumn(\"dist\",getDistToZurich(\"stop_lat\",\"stop_lon\")).filter(\"dist <= 15\")\n",
    "stops_inradius_df = stops_inradius_df.drop(\"dist\")\n",
    "#Join the stops and trips\n",
    "stops_joined_df = stop_times_df.join(stops_inradius_df,stop_times_df.stop_id == stops_inradius_df.stop_id ).drop(stops_inradius_df.stop_id)\n",
    "stops_joined_df.drop(\"parent_station\",\"location_type\",\"pickup_type\",\"drop_off_type\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write this to use it later\n",
    "#stops_joined_df.write.parquet(\"user/boecuego/stops_joined_df.parquet\")\n",
    "\n",
    "stops_joined_df = spark.read.parquet(\"user/boecuego/stops_joined_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Transforming the Data (Creating Connections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-Create Stop Connetions via Trips "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating all possible stop connections reachable by public transports. For example, if we have stops_ids `1,2,3,4` for `trip X`, then connections are 1-2,1-3,1-4,2-3,2-4,3-4 (in other words combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Collect all relevant information for each trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = stops_joined_df.groupBy(\"trip_id\").agg(F.collect_list(\"stop_id\").alias(\"stop_ids\"),F.collect_list(\"stop_sequence\").alias(\"stop_sequences\"),\\\n",
    "                                      F.collect_list(\"arrival_time\").alias(\"arrival_times\"),\\\n",
    "                                      F.collect_list(\"departure_time\").alias(\"departure_times\"),\\\n",
    "                                      )                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create combinations of these information \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.cache()\n",
    "df_grouped.take(1)\n",
    "df_grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs = df_grouped.withColumn(\"all_pairs\",pairs(\"stop_ids\",\"stop_sequences\",\"arrival_times\",\"departure_times\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             trip_id|            stop_ids|      stop_sequences|       arrival_times|     departure_times|           all_pairs|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591142...|[1, 2, 3, 4, 5, 6...|[28:45:00, 28:46:...|[28:45:00, 28:46:...|[[[8591315, 85911...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "df_pairs.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs_sep = df_pairs.withColumn(\"stop_id_pairs\",F.col(\"all_pairs\")[0])\\\n",
    "                        .withColumn(\"stop_sequence_pairs\",F.col(\"all_pairs\")[1])\\\n",
    "                        .withColumn(\"arrival_time_pairs\",F.col(\"all_pairs\")[2])\\\n",
    "                        .withColumn(\"departure_time_pairs\",F.col(\"all_pairs\")[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example this is how it looks for stop_id pairs, same applies for other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----+\n",
      "|             trip_id|      stop_id_pair|rown|\n",
      "+--------------------+------------------+----+\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591142]|   0|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8530811]|   1|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591104]|   2|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591429]|   3|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591180]|   4|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8530812]|   5|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8591364]|   6|\n",
      "|1.TA.26-18-j19-1.1.H|[8591315, 8530813]|   7|\n",
      "|1.TA.26-18-j19-1.1.H|[8591142, 8530811]|   8|\n",
      "|1.TA.26-18-j19-1.1.H|[8591142, 8591104]|   9|\n",
      "+--------------------+------------------+----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df_pairs_sep.select(\"trip_id\",F.explode(\"stop_id_pairs\").alias(\"stop_id_pair\")).withColumn(\"rown\",F.monotonically_increasing_id()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sti = df_pairs_sep.select(\"trip_id\",F.explode(\"stop_id_pairs\").alias(\"stop_id_pair\")).withColumn(\"rown\",F.monotonically_increasing_id()).alias(\"sti\")\n",
    "ssp = df_pairs_sep.select(F.explode(\"stop_sequence_pairs\").alias(\"stop_sequence_pair\"),F.monotonically_increasing_id().alias(\"rown\")).alias(\"ssp\")\n",
    "atp = df_pairs_sep.select(F.explode(\"arrival_time_pairs\").alias(\"arrival_time_pair\"),F.monotonically_increasing_id().alias(\"rown\")).alias(\"atp\")\n",
    "dtp = df_pairs_sep.select(F.explode(\"departure_time_pairs\").alias(\"departure_time_pair\"),F.monotonically_increasing_id().alias(\"rown\")).alias(\"dtp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this to run faster later\n",
    "\n",
    "#ti.cache()\n",
    "#ssp.cache()\n",
    "#atp.cache()\n",
    "#dtp.cache()\n",
    "#sti.count()\n",
    "#ssp.count()\n",
    "#atp.count()\n",
    "#dtp.count()\n",
    "\n",
    "#Uncomment this to run faster later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = sti.join(ssp,sti.rown == ssp.rown).drop(ssp.rown).join(atp,sti.rown == atp.rown).drop(atp.rown).join(dtp,sti.rown == dtp.rown).drop(dtp.rown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined.write.mode(\"overwrite\").parquet(\"user/boecuego/df_all_stop_connections.parquet\")\n",
    "joined = spark.read.parquet(\"user/boecuego/df_all_stop_connections.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+------------------+--------------------+--------------------+\n",
      "|             trip_id|        stop_id_pair|rown|stop_sequence_pair|   arrival_time_pair| departure_time_pair|\n",
      "+--------------------+--------------------+----+------------------+--------------------+--------------------+\n",
      "|1.TA.26-18-j19-1.1.H|  [8591142, 8591429]|  10|            [2, 5]|[28:46:00, 28:50:00]|[28:46:00, 28:50:00]|\n",
      "|1005.TA.26-131-j1...|[8503855:0:F, 858...| 120|            [1, 7]|[16:14:00, 16:21:00]|[16:14:00, 16:21:00]|\n",
      "|1022.TA.26-33E-j1...|  [8591230, 8591196]| 423|            [6, 7]|[18:44:00, 18:46:00]|[18:44:00, 18:46:00]|\n",
      "|1022.TA.26-33E-j1...|  [8576198, 8576182]| 485|          [14, 17]|[18:55:00, 18:59:00]|[18:55:00, 18:59:00]|\n",
      "|103.TA.26-1-A-j19...|[8503512:0:2, 850...| 500|          [14, 17]|[02:51:00, 03:00:00]|[02:51:00, 03:00:00]|\n",
      "+--------------------+--------------------+----+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to filter rows to obtain trips that are available Only on Weekdays (Monday, Tuesday, Wednesday, Thursday, Friday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stop_times\")\n",
    "stops_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stops\")\n",
    "trips_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/trips\")\n",
    "calendar_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/calendar\")\n",
    "routes_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_service  =joined.join(trips_df,trips_df.trip_id ==joined.trip_id  )\n",
    "df_days = df_service.join(calendar_df,calendar_df.service_id == df_service.service_id)\n",
    "df_weekdays = df_days.filter(\"monday == 'true' AND tuesday == 'true' AND wednesday == 'true' AND thursday == 'true' AND friday == 'true' \")\n",
    "cols = joined.columns\n",
    "df_weekdays = df_weekdays.drop(trips_df.trip_id)\n",
    "joined_filtered  = df_weekdays.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined_filtered.write.mode(\"overwrite\").parquet(\"user/boecuego/df_all_stop_connections_filtered.parquet\")\n",
    "joined_filtered = spark.read.parquet(\"user/boecuego/df_all_stop_connections_filtered.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort dataframe and selet only interested columns\n",
    "joineds = joined_filtered.sort(\"rown\").select(\"trip_id\",F.col(\"stop_id_pair\").pair1.alias(\"stop_id1\")\\\n",
    "                        ,F.col(\"stop_id_pair\").pair2.alias(\"stop_id2\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair1.alias(\"stop_sequence1\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair2.alias(\"stop_sequence2\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair1.alias(\"arrival_time1\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair2.alias(\"arrival_time2\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair1.alias(\"departure_time1\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair2.alias(\"departure_time2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have all reachable stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------+--------------+-------------+-------------+---------------+---------------+\n",
      "|             trip_id|   stop_id1|stop_id2|stop_sequence1|stop_sequence2|arrival_time1|arrival_time2|departure_time1|departure_time2|\n",
      "+--------------------+-----------+--------+--------------+--------------+-------------+-------------+---------------+---------------+\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8589111|             1|             2|     16:14:00|     16:15:00|       16:14:00|       16:15:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8573553|             1|             3|     16:14:00|     16:16:00|       16:14:00|       16:16:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8573554|             1|             4|     16:14:00|     16:18:00|       16:14:00|       16:18:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8573555|             1|             5|     16:14:00|     16:19:00|       16:14:00|       16:19:00|\n",
      "|1005.TA.26-131-j1...|8503855:0:F| 8588985|             1|             6|     16:14:00|     16:20:00|       16:14:00|       16:20:00|\n",
      "+--------------------+-----------+--------+--------------+--------------+-------------+-------------+---------------+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "joineds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2- Create Stop Connections via Walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all walkable stop pair\n",
    "\n",
    "**Conditions:**\n",
    "- If distance(stop1,stop2) < 500 meters\n",
    "- If stop in radius (15km from Zurich)\n",
    "\n",
    "**Walking speed :** 50 meters in 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_inradius_df = stops_inradius_df.drop(\"dist\")\n",
    "\n",
    "stopDist = stops_inradius_df.alias(\"stops1\").crossJoin(stops_inradius_df.alias(\"stops2\"))\\\n",
    ".where(getDist(\"stops1.stop_lat\",\"stops1.stop_lon\",\"stops2.stop_lat\",\"stops2.stop_lon\")<= 500)\\\n",
    ".withColumn(\"dist\",getDist(\"stops1.stop_lat\",\"stops1.stop_lon\",\"stops2.stop_lat\",\"stops2.stop_lon\"))\n",
    "\n",
    "\n",
    "#Other option is this but don't know if it is way more efficient or more or less similiar\n",
    "#stopDistFiltered = stops_inradius_df.alias(\"stops1\").join(stops_inradius_df.alias(\"stops2\")\\\n",
    "#,getDist(\"stops1.stop_lat\",\"stops1.stop_lon\",\"stops2.stop_lat\",\"stops2.stop_lon\")<=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "walk_stops = stopDist.select(F.col(\"stops1.stop_id\").alias(\"stop1_id\")\\\n",
    "               ,F.col(\"stops1.stop_name\").alias(\"stop1_name\")\\\n",
    "               ,F.col(\"stops1.stop_lat\").alias(\"stop1_lat\")\\\n",
    "               ,F.col(\"stops1.stop_lon\").alias(\"stop1_lon\")\\\n",
    "               ,F.col(\"stops2.stop_id\").alias(\"stop2_id\")\\\n",
    "               ,F.col(\"stops2.stop_name\").alias(\"stop2_name\")\\\n",
    "               ,F.col(\"stops2.stop_lat\").alias(\"stop2_lat\")\\\n",
    "               ,F.col(\"stops2.stop_lon\").alias(\"stop2_lon\")\\\n",
    "               ,F.col(\"dist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walk_stops.write.parquet(\"user/boecuego/walk_stops.parquet\")\n",
    "\n",
    "walk_stops = spark.read.parquet(\"user/boecuego/walk_stops.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+--------------------+------------------+\n",
      "|stop1_id|          stop1_name|   stop2_id|          stop2_name|              dist|\n",
      "+--------+--------------------+-----------+--------------------+------------------+\n",
      "| 8500926|Oetwil a.d.L., Sc...|    8500926|Oetwil a.d.L., Sc...|               0.0|\n",
      "| 8500926|Oetwil a.d.L., Sc...|    8590616|Geroldswil, Schwe...|122.61607002692374|\n",
      "| 8500926|Oetwil a.d.L., Sc...|    8590737|Oetwil an der Lim...|  300.671189772556|\n",
      "| 8502186|Dietikon Stoffelbach|    8502186|Dietikon Stoffelbach|               0.0|\n",
      "| 8502186|Dietikon Stoffelbach|8502186:0:1|Dietikon Stoffelbach| 6.761029676373361|\n",
      "+--------+--------------------+-----------+--------------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#Dist is distance between two stops\n",
    "walk_stops.drop(\"stop1_lat\",\"stop1_lon\",\"stop2_lat\",\"stop2_lon\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3- Matching Trip IDs SBB and Timetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_oneday = sbb_inradius.filter(\"betriebstag == '14.05.2019'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDiff = (F.unix_timestamp('arrival_time_real', format=\"HH:mm:ss\")\n",
    "            - F.unix_timestamp('arrival_time_schedule', format=\"HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stcweekdays = spark.read.parquet(\"user/boecuego/stcweekdays.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_oneday_english = sbb_oneday_english.withColumn(\"arrival_time\",\\\n",
    "                                                  sbb_oneday_english.schedule_arrival.substr(12,10000))\n",
    "sbb_oneday_english = sbb_oneday_english.withColumn(\"departure_time\",\\\n",
    "                                                  sbb_oneday_english.schedule_dep.substr(12,10000))\n",
    "\n",
    "sbb_oneday_english_h = sbb_oneday_english.filter(\" CAST(SUBSTRING(arrival_time,1,2) AS int) BETWEEN 7 AND 21 \")\n",
    "sbb_oneday_english_h = sbb_oneday_english_h.filter(\" CAST(SUBSTRING(departure_time,1,2) AS int) BETWEEN 7 AND 21 \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stcweekdays = stcweekdays.withColumn(\"departure_time\",\\\n",
    "                                                  stcweekdays.departure_time.substr(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(T.StringType())\n",
    "def splt(x):\n",
    "    return x.split(\":\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stopss(sbb_oneday_english_h):\n",
    "    sbb_collected_stops = sbb_oneday_english_h.groupby(\"trip_id\")\\\n",
    "            .agg(F.concat_ws(\"-\", F.collect_list(sbb_oneday_english_h.stop_id)).alias(\"stop_ids\")\\\n",
    "            ,F.concat_ws(\"-\", F.collect_list(sbb_oneday_english_h.departure_time)).alias(\"departures\")).sort(\"stop_ids\")\n",
    "    return sbb_collected_stops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_collected_stops = collect_stopss(sbb_oneday_english_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trips_collected_stops = stcweekdays.groupby(\"trip_id\")\\\n",
    "            .agg(F.concat_ws(\"-\", F.collect_list(splt(stcweekdays.stop_id))).alias(\"stop_ids\")\\\n",
    "            ,F.concat_ws(\"-\", F.collect_list(stcweekdays.departure_time)).alias(\"departures\")).sort(\"stop_ids\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_collected_stops_short = trips_collected_stops.filter((F.length(\"stop_ids\")<=len(\"8502187-8502277-8502278\"))&(F.length(\"stop_ids\")>len(\"8502208\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(T.StringType())\n",
    "def sample(sbb_stops):\n",
    "    try:\n",
    "        sbb_stops2 = \"%\"+ sbb_stops[10:-10] + \"%\"\n",
    "        return sbb_stops2\n",
    "    except:\n",
    "        return sbb_stops\n",
    "    \n",
    "@F.udf(T.StringType())\n",
    "def sample_short(sbb_stops):\n",
    "    try:\n",
    "        sbb_stops2 = \"%\"+ sbb_stops + \"%\"\n",
    "        return sbb_stops2\n",
    "    except:\n",
    "        return sbb_stops   \n",
    "    \n",
    "@F.udf(T.StringType())\n",
    "def sample_time(sbb_departure):\n",
    "    return sbb_departure[:4]+\"%\"\n",
    "\n",
    "#trips_collected_stops = trips_collected_stops.withColumn(\"sample_stop_ids\",sample(\"stop_ids\")).withColumn(\"sample_departures\",sample_time(\"departures\"))\n",
    "trips_collected_stops_short = trips_collected_stops_short.withColumn(\"sample_stop_ids\",sample_short(\"stop_ids\")).withColumn(\"sample_departures\",sample_time(\"departures\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_trips(sbb_collected_stops,trips_collected_stops):\n",
    "    \n",
    "    \n",
    "    sameStops = (sbb_collected_stops.stop_ids == trips_collected_stops.stop_ids)\n",
    "    sameDepartures =( sbb_collected_stops.departures == trips_collected_stops.departures)\n",
    "\n",
    "    trips_collected_stops = trips_collected_stops.alias(\"mts\")\n",
    "    sbb_collected_stops = sbb_collected_stops.alias(\"sbbs\")\n",
    "    matched_trips = sbb_collected_stops.join(trips_collected_stops,F.expr(\"sbbs.stop_ids LIKE mts.sample_stop_ids AND sbbs.departures LIKE mts.sample_departures\"))\n",
    "    return matched_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_trips = match_trips(sbb_collected_stops,trips_collected_stops_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_trips = matched_trips.select(F.col(\"sbbs.trip_id\").alias(\"sbb_trip_id\"),\\\n",
    "                    F.col(\"sbbs.stop_ids\").alias(\"sbb_stop_ids\"),\\\n",
    "                    F.col(\"sbbs.departures\").alias(\"sbb_departures\"),\\\n",
    "                    F.col(\"mts.trip_id\").alias(\"trips_trip_id\"),\\\n",
    "                    F.col(\"mts.stop_ids\").alias(\"trips_stop_ids\"),\\\n",
    "                    F.col(\"mts.departures\").alias(\"trips_departures\"),\\\n",
    "                    \"sample_stop_ids\",\"sample_departures\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matched_trips.write.parquet(\"user/boecuego/matched_trips2_shorts.parquet\")\n",
    "\n",
    "matched_trips = spark.read.parquet(\"user/boecuego/matched_trips2_shorts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matched_trips_shorts  = spark.read.parquet(\"user/boecuego/matched_trips2_shorts.parquet\")\n",
    "matched_trips = spark.read.parquet(\"user/boecuego/matched_trips2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Create Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonable_main = spark.read.parquet(\"user/boecuego/df_all_stop_connections_filtered.parquet\")\n",
    "walk_stops_main = spark.read.parquet(\"user/boecuego/walk_stops.parquet\")\n",
    "df_trip_network_reasonable = df_trip_network_reasonable_main\n",
    "walk_stops = walk_stops_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonable = df_trip_network_reasonable.sort(\"rown\").select(\"trip_id\",F.col(\"stop_id_pair\").pair1.alias(\"stop_id1\")\\\n",
    "                        ,F.col(\"stop_id_pair\").pair2.alias(\"stop_id2\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair1.alias(\"stop_sequence1\")\\\n",
    "                        ,F.col(\"stop_sequence_pair\").pair2.alias(\"stop_sequence2\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair1.alias(\"arrival_time1\")\\\n",
    "                        ,F.col(\"arrival_time_pair\").pair2.alias(\"arrival_time2\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair1.alias(\"departure_time1\")\\\n",
    "                        ,F.col(\"departure_time_pair\").pair2.alias(\"departure_time2\"))\n",
    "joineds = df_trip_network_reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(trip_id=u'1460.TA.26-10-j19-1.11.R', stop_id1=u'8591406', stop_id2=u'8591294', stop_sequence1=u'12', stop_sequence2=u'20', arrival_time1=u'12:27:00', arrival_time2=u'12:37:00', departure_time1=u'12:27:00', departure_time2=u'12:37:00')]"
     ]
    }
   ],
   "source": [
    "joineds = spark.read.parquet(\"user/boecuego/backtrace_df.parquet\")\n",
    "joineds.cache()\n",
    "joineds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the cost between two trip nodes:** (Arrival time of stop 2) - (Departure time of stop 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDiff = (F.unix_timestamp('arrival_time2', format=\"HH:mm:ss\")\n",
    "            - F.unix_timestamp('departure_time1', format=\"HH:mm:ss\"))\n",
    "df_trip_network_reasonable = df_trip_network_reasonable.withColumn(\"cost\",F.ceil(timeDiff/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the cost between two walking nodes:** Distance/50 + 2minutes transfer delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_stops = walk_stops.withColumn(\"cost\", F.ceil(F.col(\"dist\")/50)+2)\n",
    "walk_stops = walk_stops.select(F.col('stop1_id').alias('stop_id1'), F.col('stop2_id').alias('stop_id2'), 'cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonables =  df_trip_network_reasonable.select(\"stop_id1\",\"stop_id2\",\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_network_reasonables=df_trip_network_reasonables.withColumn(\"type\",F.lit(\"trip\"))\n",
    "walk_stops = walk_stops.withColumn(\"type\",F.lit(\"walk\"))\n",
    "unioned_df = df_trip_network_reasonables.union(walk_stops)\n",
    "unioned_df = unioned_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asumption for simplicity:** If there are multiple edges between two nodes we only get the edge with minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df_unique = unioned_df.groupBy(\"stop_id1\",\"stop_id2\").agg(F.min(\"cost\").alias(\"cost\"),F.collect_set(\"type\").alias(\"types\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asumption:** We want to penalize using too many transfers so we add extra 10minutes of cost to each edge\n",
    "\n",
    "- A --> B --> C\n",
    "- A --> C\n",
    "\n",
    "we prioritize A -->C\n",
    "We are also doing this because these two routes may belong to same trip and we don't want the intermediate stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unioned_df_unique = unioned_df_unique.withColumn(\"cost\",F.col(\"cost\")+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unioned_df_unique = spark.read.parquet(\"user/boecuego/graph_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd_df = unioned_df_unique.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph with networkx, we store cost (journey duration in minutes + penalty) and types (walking or public transport) in the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mygraph = nx.from_pandas_edgelist(pd_df, 'stop_id1', 'stop_id2',[\"cost\",\"types\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Find the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find the k shortest paths\n",
    "def k_shortest_paths(G, source, target, k, weight):\n",
    "    return list(islice(nx.shortest_simple_paths(G, source, target, weight=weight), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8580301"
     ]
    }
   ],
   "source": [
    "for node in mygraph.nodes:\n",
    "    print(node)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find **k** shortest path between two stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src = \"8503000\" #\"8591184\"  \n",
    "tgt  = \"8591049\" #\"8591244\" \n",
    "\n",
    "src_main = src.split(\":\")[0]\n",
    "tgt_main = tgt.split(\":\")[0]\n",
    "counter = 0\n",
    "paths = []\n",
    "for node in mygraph.nodes:\n",
    "    if src_main in node:\n",
    "        counter+=1\n",
    "        \n",
    "if counter > 1:\n",
    "    k = 5\n",
    "else:\n",
    "    k=50\n",
    "for node in mygraph.nodes:\n",
    "    if src_main in node:\n",
    "        p = k_shortest_paths(mygraph, node, tgt,k,weight = \"cost\")\n",
    "        mygraph[src_main][node][\"cost\"] = 0\n",
    "        for x in p:\n",
    "            paths.append(x)\n",
    "\n",
    "\n",
    "#paths = k_shortest_paths(mygraph, src_main, tgt_main,5,weight = \"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120"
     ]
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#This is a helper to get pairs in a list\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return izip(a, b)\n",
    "#When converting a date to string this is a helper\n",
    "def append_zero(x):\n",
    "    if len(str(x)) == 1:\n",
    "        return \"0\"+str(x)\n",
    "    return str(x)\n",
    "#Helper to remove adjecent walks\n",
    "def remove_adjecent_walks(paths):\n",
    "    no_adjecentwalk =[]\n",
    "    for pr in paths:\n",
    "        check = True\n",
    "        for p1,p2 in pairwise(pairwise(pr)):\n",
    "\n",
    "            if mygraph[p1[0]][p1[1]][\"types\"] == [\"walk\"] and mygraph[p2[0]][p2[1]][\"types\"] == [\"walk\"]:\n",
    "                #print(counter)\n",
    "                check = False\n",
    "\n",
    "        if check:\n",
    "            no_adjecentwalk.append(pr)\n",
    "    return no_adjecentwalk\n",
    "\n",
    "#Helper to reverse route\n",
    "def get_reverse_route(paths):\n",
    "    reversed_routes  = []\n",
    "    for route in paths:\n",
    "        lst = []\n",
    "        for a in pairwise(route):\n",
    "            lst.append(a)\n",
    "        lst_reversed = lst[::-1]\n",
    "        reversed_routes.append(lst_reversed)\n",
    "    return reversed_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want two consecutive walks, so filter them if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_adjecentwalk = remove_adjecent_walks(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know arrival time we need to **reverse** the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reversed_routes  = get_reverse_route(no_adjecentwalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is arrival_time\n",
    "current_time = datetime.datetime(2019,1,1,12,30,0,0)\n",
    "cut_off_time = datetime.datetime(2019,1,1,11,40,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorihm:** Starting from the last connection, we try to find most suitable  trips (if not possible walks) by comparing from timetables matching the arrival time and backtrace from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notPossible = datetime.datetime(1999,1,1,12,12,0,0)\n",
    "def find_possible_routes(reversed_routes,current_timez,cut_off_time):\n",
    "    \n",
    "    shorts = []\n",
    "    longs = []\n",
    "    printouts = []\n",
    "    notpossibles = []\n",
    "    for route in reversed_routes:\n",
    "        break_loop = False\n",
    "        current_time = current_timez\n",
    "        current_trip_id = \"\"\n",
    "        \n",
    "        route_with_timetable = []\n",
    "        trip = []\n",
    "        prints  = []\n",
    "        actual_prints=[]\n",
    "        for p1,p2 in route:\n",
    "            #p2 arrival\n",
    "            #p1 departure\n",
    "            if mygraph[p1][p2][\"types\"] == [\"walk\"]:\n",
    "                prints.append(\"Transfer with walking\")\n",
    "                prints.append((\"after_walk\",current_time))\n",
    "                cost = mygraph[p1][p2][\"cost\"]\n",
    "                if cost > 0:\n",
    "                    current_time  = current_time - datetime.timedelta(minutes=cost-10)#-2\n",
    "                else:\n",
    "                    current_time = current_time\n",
    "                prints.append((\"before_walk\",current_time))\n",
    "                current_trip_id=\"\"\n",
    "                \n",
    "                actual_prints.append(\"Walk from \"+p1+\" ----to----> \"+p2+\" for \"+str(cost-10)+\" minutes.\")\n",
    "                prints.append((p1,p2))\n",
    "                continue\n",
    "            else:\n",
    "                #Get current times hour and minute\n",
    "                hour = current_time.hour\n",
    "                minute = current_time.minute\n",
    "\n",
    "                #Convert it to string\n",
    "                strtime = append_zero(hour)+\":\"+append_zero(minute)+\":00\"\n",
    "\n",
    "                #Get closest trip to current time with matching stop_ids\n",
    "                if current_trip_id != \"\":\n",
    "                    trip = joineds.filter(F.col(\"trip_id\") == current_trip_id)\\\n",
    "                    .filter(\"stop_id1 == '\"+p1+\"'\"+\" AND \"+\"stop_id2 == '\"+p2+\"'\").take(1)\n",
    "               #trip = df_trip_network_reasonable2.filter(\"prev_stop_id == '\"+p1+\"'\"+\" AND \"+\"stop_id == '\"+p2+\"'\").filter(\"arrival_time <='\"+strtime+\"'\").sort(F.desc(\"arrival_time\")).take(1)[0]\n",
    "\n",
    "                #If previous trip_id was different from this trip's id then put 2 minute waiting time\n",
    "                if trip == [] or current_trip_id == \"\":\n",
    "\n",
    "                    #2 min waiting\n",
    "                    if current_trip_id != \"\":\n",
    "                        current_time = current_time - datetime.timedelta(minutes = 2)\n",
    "                        prints.append(\"Transfer\")\n",
    "\n",
    "\n",
    "                    #Get hours, minutes\n",
    "                    hour = current_time.hour\n",
    "                    minute = current_time.minute\n",
    "                    strtime = append_zero(hour)+\":\"+append_zero(minute)+\":00\"\n",
    "\n",
    "                    #With new current time get new trip\n",
    "                    trip = joineds.filter(\"stop_id1 == '\"+p1+\"'\"+\" AND \"+\"stop_id2 == '\"+p2+\"'\").filter(\"arrival_time2 <='\"+strtime+\"'\").sort(F.desc(\"arrival_time2\")).take(1)\n",
    "                    #update current trip_id\n",
    "                try:\n",
    "                    trip = trip[0]\n",
    "                except:\n",
    "                    #print(\"Encountered a route that is not possible currently\",(p1,p2))\n",
    "                    current_time = notPossible\n",
    "                    break\n",
    "                current_trip_id  = trip.trip_id\n",
    "\n",
    "                #Update current time\n",
    "                prtstr = \"Take trip: \"+trip.trip_id+\" from \"+p1+\" departing at \"+trip.departure_time1+\" ------------> to \"+p2+\" arriving at \"+trip.arrival_time2\n",
    "                actual_prints.append(prtstr)\n",
    "                prints.append((trip.trip_id,trip.arrival_time1,trip.departure_time1,trip.arrival_time2,trip.departure_time2))\n",
    "                hms = trip.departure_time1.split(\":\")\n",
    "                h = int(hms[0])\n",
    "                m = int(hms[1])\n",
    "                current_time = datetime.datetime(2019,1,1,h,m,0,0)\n",
    "\n",
    "            prints.append((p1,p2))\n",
    "            if break_loop:\n",
    "                prints = []\n",
    "                break\n",
    "        if current_time >= cut_off_time:\n",
    "            shorts.append(prints)\n",
    "            shorts.append(current_time)\n",
    "            shorts.append(\"---------------------------------------------------------------\")\n",
    "            printouts.append(actual_prints)\n",
    "        elif current_time == notPossible:\n",
    "            notpossibles.append(prints)\n",
    "            notpossibles.append(current_time)\n",
    "            notpossibles.append(\"---------------------------------------------------------------\")\n",
    "        else:\n",
    "            longs.append(prints)\n",
    "            longs.append(\"---------------------------------------------------------------\")\n",
    "    return shorts,longs,printouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "good_routes,bad_routes,printouts = find_possible_routes(reversed_routes,current_time,cut_off_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:41/42', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:41/42', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:7', u'8580449')\n",
      "(u'235.TA.26-15-j19-1.41.H', u'11:50:00', u'11:52:00', u'11:58:00', u'11:59:00')\n",
      "(u'8503000:0:41/42', u'8503006:0:7')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'168.TA.26-12-A-j19-1.2.H', u'12:23:00', u'12:23:00', u'12:29:00', u'12:29:00')\n",
      "(u'8590620', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 23))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 19))\n",
      "(u'8503310:0:3', u'8590620')\n",
      "(u'20.TA.26-9-A-j19-1.2.H', u'12:02:00', u'12:07:00', u'12:17:00', u'12:18:00')\n",
      "(u'8503000:0:41/42', u'8503310:0:3')\n",
      "2019-01-01 12:07:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:7', u'8580449')\n",
      "(u'235.TA.26-15-j19-1.41.H', u'11:50:00', u'11:52:00', u'11:58:00', u'11:59:00')\n",
      "(u'8503000:0:41/42', u'8503006:0:7')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "(u'8503000', u'8503000:0:41/42')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'529.TA.26-12-A-j19-1.3.R', u'12:12:00', u'12:12:00', u'12:16:00', u'12:16:00')\n",
      "(u'8587655', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 12))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 9))\n",
      "(u'8503129:0:3', u'8587655')\n",
      "(u'528.TA.26-8-A-j19-1.344.H', u'11:53:00', u'11:55:00', u'12:03:00', u'12:04:00')\n",
      "(u'8503000:0:34', u'8503129:0:3')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:2', u'8580449')\n",
      "(u'528.TA.26-8-A-j19-1.344.H', u'11:53:00', u'11:55:00', u'11:59:00', u'12:00:00')\n",
      "(u'8503000:0:34', u'8503006:0:2')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 17))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 10))\n",
      "(u'8503006:0:2', u'8591382')\n",
      "(u'528.TA.26-8-A-j19-1.344.H', u'11:53:00', u'11:55:00', u'11:59:00', u'12:00:00')\n",
      "(u'8503000:0:34', u'8503006:0:2')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:5', u'8580449')\n",
      "(u'88.TA.26-36-j19-1.48.R', u'11:49:00', u'11:52:00', u'11:57:00', u'11:57:00')\n",
      "(u'8503000:0:33', u'8503006:0:5')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'529.TA.26-12-A-j19-1.3.R', u'12:12:00', u'12:12:00', u'12:16:00', u'12:16:00')\n",
      "(u'8587655', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 12))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 9))\n",
      "(u'8503129:0:3', u'8587655')\n",
      "(u'64.TA.26-19-j19-1.17.H', u'11:49:00', u'11:49:00', u'11:58:00', u'11:59:00')\n",
      "(u'8503000:0:32', u'8503129:0:3')\n",
      "2019-01-01 11:49:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:32', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:2', u'8580449')\n",
      "(u'64.TA.26-19-j19-1.17.H', u'11:49:00', u'11:49:00', u'11:54:00', u'11:55:00')\n",
      "(u'8503000:0:32', u'8503006:0:2')\n",
      "2019-01-01 11:49:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:31', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:15', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:15', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "(u'8503000:0:15', u'8591067')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:14', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:14', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:17', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:17', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:16', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:16', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:11', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:11', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:10', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:10', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:13', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:13', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:12', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:12', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:18', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:18', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000P', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000P', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 53))\n",
      "(u'8503000P', u'8591067')\n",
      "2019-01-01 11:53:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:17:00', u'12:17:00')\n",
      "(u'8587349', u'8591382')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000P', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'46.TA.26-79-j19-1.1.R', u'12:18:00', u'12:18:00', u'12:22:00', u'12:22:00')\n",
      "(u'8591349', '8591049')\n",
      "Transfer\n",
      "(u'1183.TA.26-7-B-j19-1.6.R', u'11:59:00', u'11:59:00', u'12:12:00', u'12:12:00')\n",
      "(u'8591174', u'8591349')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 49))\n",
      "(u'8503000P', u'8591174')\n",
      "2019-01-01 11:49:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:9', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:9', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:8', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:8', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 51))\n",
      "(u'8503000:0:8', u'8591067')\n",
      "2019-01-01 11:51:00\n",
      "---------------------------------------------------------------\n",
      "(u'46.TA.26-79-j19-1.1.R', u'12:18:00', u'12:18:00', u'12:22:00', u'12:22:00')\n",
      "(u'8591349', '8591049')\n",
      "Transfer\n",
      "(u'1183.TA.26-7-B-j19-1.6.R', u'11:59:00', u'11:59:00', u'12:12:00', u'12:12:00')\n",
      "(u'8591174', u'8591349')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 50))\n",
      "(u'8503000:0:8', u'8591174')\n",
      "2019-01-01 11:50:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:17:00', u'12:17:00')\n",
      "(u'8587349', u'8591382')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:8', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:3', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:3', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:6', u'8580449')\n",
      "(u'651.TA.26-24-j19-1.256.R', u'11:39:00', u'11:44:00', u'11:51:00', u'11:52:00')\n",
      "(u'8503000:0:3', u'8503006:0:6')\n",
      "2019-01-01 11:44:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:5', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:5', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:15:00', u'12:15:00', u'12:24:00', u'12:24:00')\n",
      "(u'8580449', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 15))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 12, 11))\n",
      "(u'8503006:0:6', u'8580449')\n",
      "(u'32.TA.80-159-Y-j19-1.8.H', u'12:05:00', u'12:05:00', u'12:11:00', u'12:11:00')\n",
      "(u'8503000:0:5', u'8503006:0:6')\n",
      "2019-01-01 12:05:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:4', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 54))\n",
      "(u'8503000:0:4', u'8587349')\n",
      "2019-01-01 11:54:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:7', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:7', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'11:59:00', u'11:59:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591067', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 52))\n",
      "(u'8503000:0:7', u'8591067')\n",
      "2019-01-01 11:52:00\n",
      "---------------------------------------------------------------\n",
      "(u'46.TA.26-79-j19-1.1.R', u'12:18:00', u'12:18:00', u'12:22:00', u'12:22:00')\n",
      "(u'8591349', '8591049')\n",
      "Transfer\n",
      "(u'1183.TA.26-7-B-j19-1.6.R', u'11:59:00', u'11:59:00', u'12:12:00', u'12:12:00')\n",
      "(u'8591174', u'8591349')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 11, 59))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 50))\n",
      "(u'8503000:0:7', u'8591174')\n",
      "2019-01-01 11:50:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:17:00', u'12:17:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591382', '8591049')\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:17:00', u'12:17:00')\n",
      "(u'8587349', u'8591382')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:7', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:6', u'8587349')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:6', u'8591379')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:01:00', u'12:01:00', u'12:24:00', u'12:24:00')\n",
      "(u'8587349', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 1))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 56))\n",
      "(u'8503000:0:43/44', u'8587349')\n",
      "2019-01-01 11:56:00\n",
      "---------------------------------------------------------------\n",
      "(u'1914.TA.26-11-A-j19-1.27.R', u'12:03:00', u'12:03:00', u'12:24:00', u'12:24:00')\n",
      "(u'8591379', '8591049')\n",
      "Transfer with walking\n",
      "('after_walk', datetime.datetime(2019, 1, 1, 12, 3))\n",
      "('before_walk', datetime.datetime(2019, 1, 1, 11, 55))\n",
      "(u'8503000:0:43/44', u'8591379')\n",
      "2019-01-01 11:55:00\n",
      "---------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "for element in good_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in bad_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best route(s)\n",
    "\n",
    "**Assumption:** Best route(s) is(are) the route(s) with latest departure(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxt = datetime.datetime(2019, 1, 1, 0,0)\n",
    "\n",
    "index = -1\n",
    "start_times  = []\n",
    "for i,t in enumerate(good_routes):\n",
    "    if type(t) == type(datetime.datetime(2019, 1, 1, 11, 56)):\n",
    "        start_times.append((t,i))\n",
    "        if t > maxt:\n",
    "            maxt = t\n",
    "            index = i\n",
    "start_times_reversed = sorted(start_times, key=lambda tup: tup[0],reverse=True)\n",
    "best_routes = []\n",
    "best_routes_print = []\n",
    "for time,index in start_times_reversed[:10]:\n",
    "    route = good_routes[index-1:index]\n",
    "    best_routes.append(route[0])\n",
    "    route_print = printouts[index/3]\n",
    "    best_routes_print.append(route_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print best route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we did backtracing to find the best routes, we should read this data from end to start. For example, the passanger first takes the trip ID `'629.TA.26-32-j19-1.10.R'` in order to go from the station ID `8591184` to station ID `8591101`. The first time entry is the arrival time to the previous stop (in this case the stop with the ID `8591184`), the second time entry is the departure time from the previous stop. The third time entry is the arrival time to the current stop (in this case the stop with the ID `8591101`), the fourth time entry is the departure time from the current stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Take trip: 168.TA.26-12-A-j19-1.2.H from 8590620 departing at 12:23:00 ------------> to 8591049 arriving at 12:29:00\n",
      "Walk from 8503310:0:3 ----to----> 8590620 for 4 minutes.\n",
      "Take trip: 20.TA.26-9-A-j19-1.2.H from 8503000:0:41/42 departing at 12:07:00 ------------> to 8503310:0:3 arriving at 12:17:00\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8580449 departing at 12:15:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503006:0:6 ----to----> 8580449 for 4 minutes.\n",
      "Take trip: 32.TA.80-159-Y-j19-1.8.H from 8503000:0:5 departing at 12:05:00 ------------> to 8503006:0:6 arriving at 12:11:00\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8587349 departing at 12:01:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000:0:41/42 ----to----> 8587349 for 5 minutes.\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8587349 departing at 12:01:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000 ----to----> 8587349 for 5 minutes.\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000:0:32 ----to----> 8591379 for 7 minutes.\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000:0:31 ----to----> 8591379 for 7 minutes.\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8587349 departing at 12:01:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000:0:14 ----to----> 8587349 for 5 minutes.\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000:0:17 ----to----> 8591379 for 7 minutes.\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000:0:11 ----to----> 8591379 for 7 minutes.\n",
      "************************************************************\n",
      "Take trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "Walk from 8503000:0:10 ----to----> 8591379 for 7 minutes."
     ]
    }
   ],
   "source": [
    "for r in best_routes_print:\n",
    "    print(\"******\"*10)\n",
    "    for x in r:\n",
    "        print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df = spark.read.orc(\"hdfs:///data/sbb/timetables/orc/stops\")\n",
    "stops_inradius_df= stops_df.withColumn(\"dist\",getDistToZurich(\"stop_lat\",\"stop_lon\")).filter(\"dist <= 15\")\n",
    "\n",
    "stops_inradius2 = set(stops_inradius_df.select(\"stop_id\").distinct().rdd.map(lambda r: r[0]).collect())\n",
    "stops_inradius = stops_inradius2.copy()\n",
    "for s in stops_inradius2:\n",
    "    if \":\" in s:\n",
    "        stops_inradius.add(s.split(\":\")[0])\n",
    "sbb = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "sbb_inradius = sbb.where(F.col(\"bpuic\").isin(stops_inradius))\n",
    "\n",
    "sbb_inradius_weekdays = sbb_inradius.withColumn('date', F.from_unixtime(F.unix_timestamp('betriebstag', 'dd.MM.yyy')))\\\n",
    "    .withColumn(\"day\",F.dayofweek(\"date\"))\\\n",
    "        .filter(\"day BETWEEN 2 AND 6\")\n",
    "\n",
    "\n",
    "# rename the fields german -> english\n",
    "fields = {\n",
    "    'BETRIEBSTAG':'date',\n",
    "    'FAHRT_BEZEICHNER':'trip_id',\n",
    "    'PRODUKT_ID':'transport_type',\n",
    "    'LINIEN_ID':'train_id',\n",
    "    'LINIEN_TEXT':'line',\n",
    "    'VERKEHRSMITTEL_TEXT':'train_type',\n",
    "    'ZUSATZFAHRT_TF':'additional_trip',\n",
    "    'FAELLT_AUS_TF':'trip_failed',\n",
    "    'HALTESTELLEN_NAME':'stop_name',\n",
    "    'BPUIC':'stop_id',\n",
    "    'ANKUNFTSZEIT':'schedule_arrival',\n",
    "    'AN_PROGNOSE':'real_arrival',\n",
    "    'AN_PROGNOSE_STATUS':'arr_forecast_status',\n",
    "    'ABFAHRTSZEIT':'schedule_dep',\n",
    "    'AB_PROGNOSE':'real_dep',\n",
    "    'AB_PROGNOSE_STATUS':'dep_forecast_status',\n",
    "    'DURCHFAHRT_TF':'no_stop_here'\n",
    "}\n",
    "sbb_inradius_weekdays_english = sbb_inradius_weekdays.selectExpr([k + ' as ' + fields[k] for k in fields])\n",
    "\n",
    "sbb_inradius_weekdays_english = sbb_inradius_weekdays_english.withColumn(\"arrival_time_schedule\",\\\n",
    "                                                  sbb_inradius_weekdays_english.schedule_arrival.substr(12,10000))\\\n",
    "                                                    .withColumn(\"departure_time_schedule\",\\\n",
    "                                                  sbb_inradius_weekdays_english.schedule_dep.substr(12,10000))\\\n",
    "                                                    .withColumn(\"arrival_time_real\",\\\n",
    "                                                  sbb_inradius_weekdays_english.real_arrival.substr(12,10000))\\\n",
    "                                                    .withColumn(\"departure_time_real\",\\\n",
    "                                                  sbb_inradius_weekdays_english.real_dep.substr(12,10000))\n",
    "#sbb_inradius_weekdays_english.write.parquet(\"user/boecuego/sbb_inradius_weekdays_english.parquet\")\n",
    "\n",
    "\n",
    "sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_weekdays_english.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only stop id's also exist between 13-17 May 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the trip_ids available only between 13-17 May\n",
    "\n",
    "\n",
    "sbb_inradius_13_17_trips = sbb_inradius.filter(\"date <= '17.09.2019' AND date >= '13.09.2019' \").select(\"trip_id\").distinct()\n",
    "sbb_inradius_13_17_trips_select = sbb_inradius.join(F.broadcast(sbb_inradius_13_17_trips), sbb_inradius.trip_id == sbb_inradius_13_17_trips.trip_id)\n",
    "sbb_inradius_13_17_trips_select = sbb_inradius_13_17_trips_select.drop(sbb_inradius.trip_id)\n",
    "#sbb_inradius_13_17_trips_select.write.parquet(\"user/boecuego/sbb_inradius_13_17_trips_select2.parquet\")\n",
    "\n",
    "sbb_inradius_13_17_trips_select = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_select2.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect also stop_ids of each trip \n",
    "\n",
    "For example -> trip_id1 : [stopid1,stopid2,...stopid7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_trip_stop_ids = sbb_inradius_13_17_trips_select.groupBy(\"trip_id\",\"date\")\\\n",
    ".agg(F.collect_list(\"stop_id\").alias(\"stop_ids\"))\\\n",
    ".groupBy(\"trip_id\").agg(F.first(\"stop_ids\").alias(\"stop_ids\"))\n",
    "#sbb_trip_stop_ids.write.parquet(\"user/boecuego/sbb_trip_stop_ids2.parquet\")\n",
    "sbb_trip_stop_ids = spark.read.parquet(\"user/boecuego/sbb_trip_stop_ids2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_inradius_13_17_trips_wstops = sbb_inradius_13_17_trips_select\\\n",
    ".join(F.broadcast(sbb_trip_stop_ids), sbb_inradius_13_17_trips_select.trip_id == sbb_trip_stop_ids.trip_id)\n",
    "sbb_inradius_13_17_trips_wstops = sbb_inradius_13_17_trips_wstops.drop(sbb_inradius_13_17_trips_select.trip_id)\n",
    "#sbb_inradius_13_17_trips_wstops.write.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\")\n",
    "\n",
    "sbb_inradius_13_17_trips_wstops = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sbb_inradius.write.parquet(\"user/boecuego/sbb_inradius_weekdays_engl.parquet\")\n",
    "#####\n",
    "#sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_weekdays_engl.parquet\") #this is weekdays\n",
    "###\n",
    "#sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_select.parquet\") #this is only trip_ids from 13_17\n",
    "sbb_inradius_wostops = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\")\n",
    "sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_13_17_trips_wstops2.parquet\") #Also added stop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sbb_grouped_delay = sbb_inradius_wostops.withColumn(\"delay\",F.floor(timeDiff/60)).groupBy(\"stop_id\",\"delay\").count()\n",
    "# sbb_grouped_delay.write.parquet(\"user/boecuego/sbb_stops_time_grouped.parquet\")\n",
    "\n",
    "sbb_grouped_delay = spark.read.parquet(\"user/boecuego/sbb_stops_time_grouped.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\ndef find_prob_wprevstop(stop_id,arrival_time_schedule,prev_stop_id):\\n    timeDiff = (F.unix_timestamp(\\'arrival_time_real\\', format=\"HH:mm:ss\")\\n            - F.unix_timestamp(\\'arrival_time_schedule\\', format=\"HH:mm\"))\\n    delays = sbb_inradius.filter((F.col(\"stop_id\") == stop_id)     & (time_interval_udf(\"arrival_time_schedule\",F.lit(arrival_time_schedule)))     & (F.array_contains(sbb_inradius.stop_ids,prev_stop_id ))).withColumn(\"delay\",F.floor(timeDiff/60)).groupBy(\"delay\").count()\\n    #Find the total num\\n    #total_num = delays.groupBy().sum(\"count\").collect()[0][0]\\n    #print(total_num)\\n    delays_pd = delays.toPandas()\\n    total_num = sum(delays_pd[\"count\"].values)\\n    delays_pd[\"percentage\"] = delays_pd[\"count\"].map(lambda x:(x/float(total_num))*100)\\n    return delays_pd    \\n'"
     ]
    }
   ],
   "source": [
    "#!!! TO DO !!!!!\n",
    "#Filter weekends, public holidays etc\n",
    "#!!! TO DO !!!!!\n",
    "def find_prob(stop_id,arrival_time_schedule):\n",
    "    timeDiff = (F.unix_timestamp('arrival_time_real', format=\"HH:mm:ss\")\n",
    "            - F.unix_timestamp('arrival_time_schedule', format=\"HH:mm\"))\n",
    "    delays = sbb_grouped_delay.filter((F.col('stop_id') == stop_id) & time_interval_udf(F.col('arrival_time_schedule'), F.lit(arrival_time_schedule)))\n",
    "    #Find the total num\n",
    "    #total_num = delays.groupBy().sum(\"count\").collect()[0][0]\n",
    "    #print(total_num)\n",
    "    delays_pd = delays.toPandas()\n",
    "    total_num = sum(delays_pd[\"count\"].values)\n",
    "    delays_pd[\"percentage\"] = delays_pd[\"count\"].map(lambda x:(x/float(total_num))*100)\n",
    "    return delays_pd\n",
    "    \n",
    "\"\"\"\n",
    "def find_prob_wprevstop(stop_id,arrival_time_schedule,prev_stop_id):\n",
    "    timeDiff = (F.unix_timestamp('arrival_time_real', format=\"HH:mm:ss\")\n",
    "            - F.unix_timestamp('arrival_time_schedule', format=\"HH:mm\"))\n",
    "    delays = sbb_inradius.filter((F.col(\"stop_id\") == stop_id) \\\n",
    "    & (time_interval_udf(\"arrival_time_schedule\",F.lit(arrival_time_schedule))) \\\n",
    "    & (F.array_contains(sbb_inradius.stop_ids,prev_stop_id ))).withColumn(\"delay\",F.floor(timeDiff/60)).groupBy(\"delay\").count()\n",
    "    #Find the total num\n",
    "    #total_num = delays.groupBy().sum(\"count\").collect()[0][0]\n",
    "    #print(total_num)\n",
    "    delays_pd = delays.toPandas()\n",
    "    total_num = sum(delays_pd[\"count\"].values)\n",
    "    delays_pd[\"percentage\"] = delays_pd[\"count\"].map(lambda x:(x/float(total_num))*100)\n",
    "    return delays_pd    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find probabiliy for the good routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each transfer, we calculate the probability of not missing the transfer vehicle. Assume that a passanger does $n$ transfers for the planned journey. Then, the probability of arriving in time is:\n",
    "\n",
    "\\begin{align}\n",
    "Pr[\\textrm{Arriving in time with the found route}] & = Pr[\\textrm{Not missing any transfers}] \\cdot Pr[\\textrm{Arriving the final station in time}] \\\\\n",
    "                                                   & = \\prod_{i=0}^{n-1} Pr[\\textrm{Not missing transfer i}] \\cdot Pr[\\textrm{Arriving the final station in time}]\n",
    "\\end{align}\n",
    "\n",
    "where the second equality follows from the assumption that **the delays of transports are independent of each other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confidence_level = 0.88\n",
    "def calculate_probabilities(desired_arrival_time,best_routes, best_routes_print,confidence_level):\n",
    "    counter = 0\n",
    "    for r,printroute in zip(best_routes,best_routes_print):\n",
    "        overall_prob = 1.\n",
    "        \n",
    "        printornot = []\n",
    "        for i in range(len(r)):\n",
    "            # This case corresponds to the Pr[Arriving the final station in time] case\n",
    "            # That is why the index is zero and the final means of travelling should not be walking but a public transport\n",
    "            if i == 0 and r[i]!=\"Transfer with walking\":\n",
    "                las_arrival_time = r[i][3]\n",
    "                ah = las_arrival_time.split(\":\")[0]\n",
    "                am = las_arrival_time.split(\":\")[1]\n",
    "                a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "                last_station = r[i+1][1]\n",
    "                \n",
    "                allowed_delay = (desired_arrival_time-a).total_seconds()/60\n",
    "                prob_dist =find_prob(last_station,las_arrival_time[:-3])\n",
    "                probability_to_catch = sum(prob_dist[prob_dist[\"delay\"]<= allowed_delay].percentage.values)\n",
    "                overall_prob = (float(overall_prob)*float(probability_to_catch))/100.\n",
    "                printornot.append(\"Prob to arrive on time: \"+str(probability_to_catch))\n",
    "\n",
    "            try:\n",
    "                if r[i] == \"Transfer with walking\":\n",
    "\n",
    "                    transfer_departure_time = r[i-2][2]\n",
    "                    dh = transfer_departure_time.split(\":\")[0]\n",
    "                    dm = transfer_departure_time.split(\":\")[1]\n",
    "                    d = datetime.datetime(2019, 1, 1, int(dh), int(dm))\n",
    "                    #think this\n",
    "                    transfer_station =  r[i+5][1].split(\":\")[0]\n",
    "\n",
    "                    walking_min = (r[i+1][1] - r[i+2][1]).total_seconds()/60\n",
    "\n",
    "                    #think this\n",
    "                    transfer_arrival_time = r[i+4][3]\n",
    "            \n",
    "\n",
    "                    ah = transfer_arrival_time.split(\":\")[0]\n",
    "                    am = transfer_arrival_time.split(\":\")[1]\n",
    "                    a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "                    allowed_delay = (d-a).total_seconds()/60 - walking_min\n",
    "                    #print(transfer_station,transfer_arrival_time)\n",
    "                    prob_dist =find_prob(transfer_station,transfer_arrival_time[:-3])\n",
    "\n",
    "                    probability_to_catch = sum(prob_dist[prob_dist[\"delay\"]<= allowed_delay].percentage.values)\n",
    "                    \n",
    "                    printornot.append(\"Probability to catch trip \"+r[i-2][0]+\" with \"+str(walking_min)+\" min walking  : \"+str(probability_to_catch))\n",
    "                    \n",
    "                    overall_prob = (float(overall_prob)*float(probability_to_catch))/100.\n",
    "                \n",
    "                elif r[i] == \"Transfer\":\n",
    "                    transfer_departure_time = r[i-2][2]\n",
    "                    dh = transfer_departure_time.split(\":\")[0]\n",
    "                    dm = transfer_departure_time.split(\":\")[1]\n",
    "                    d = datetime.datetime(2019, 1, 1, int(dh), int(dm))\n",
    "                    transfer_station =  r[i-1][1]\n",
    "                    \n",
    "                    transfer_arrival_time = r[i+1][3]\n",
    "                    \n",
    "                    ah = transfer_arrival_time.split(\":\")[0]\n",
    "                    am = transfer_arrival_time.split(\":\")[1]\n",
    "                    a = datetime.datetime(2019, 1, 1, int(ah), int(am))\n",
    "                    allowed_delay = (d-a).total_seconds()/60\n",
    "                    prob_dist =find_prob(transfer_station,transfer_arrival_time[:-3])\n",
    "                    \n",
    "                    probability_to_catch = sum(prob_dist[prob_dist[\"delay\"]<= allowed_delay].percentage.values)\n",
    "                    printornot.append(\"Probability to catch trip \"+r[i-2][0]+\" : \"+str(probability_to_catch))\n",
    "                    overall_prob = (float(overall_prob)*float(probability_to_catch))/100.\n",
    "\n",
    "            except:\n",
    "                print(\"-\")\n",
    "        \n",
    "        if overall_prob > confidence_level:\n",
    "            counter +=1\n",
    "            print(\"*\"*100)\n",
    "            print(\"Overal probability: \"+str(overall_prob))\n",
    "            print(\"Probabilities: \")\n",
    "            for prt in printornot:\n",
    "                print \"\\t\"+prt\n",
    "            print(\"Route: \")\n",
    "            for rou in printroute:\n",
    "                print \"\\t\"+rou\n",
    "            #print(\"Probability matches the confidence level, we can select this route.\")\n",
    "        else:\n",
    "            warning = 1\n",
    "            #for rou in printroute:\n",
    "            #    print rou\n",
    "            #print(\"WARNING: Confidence level is lower than the required amount! Do not select this route!\")\n",
    "    print str(counter)+\" routes found out of \"+str(len(best_routes))+\" best routes , if you want more please enter lower confidence\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Overal probability: 0.957257798926\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 97.2543352601156\n",
      "\tProbability to catch trip 168.TA.26-12-A-j19-1.2.H with 4.0 min walking  : 98.42829076620825\n",
      "Route: \n",
      "\tTake trip: 168.TA.26-12-A-j19-1.2.H from 8590620 departing at 12:23:00 ------------> to 8591049 arriving at 12:29:00\n",
      "\tWalk from 8503310:0:3 ----to----> 8590620 for 4 minutes.\n",
      "\tTake trip: 20.TA.26-9-A-j19-1.2.H from 8503000:0:41/42 departing at 12:07:00 ------------> to 8503310:0:3 arriving at 12:17:00\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8587349 departing at 12:01:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000:0:41/42 ----to----> 8587349 for 5 minutes.\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8587349 departing at 12:01:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000 ----to----> 8587349 for 5 minutes.\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000:0:32 ----to----> 8591379 for 7 minutes.\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000:0:31 ----to----> 8591379 for 7 minutes.\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8587349 departing at 12:01:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000:0:14 ----to----> 8587349 for 5 minutes.\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000:0:17 ----to----> 8591379 for 7 minutes.\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000:0:11 ----to----> 8591379 for 7 minutes.\n",
      "-\n",
      "****************************************************************************************************\n",
      "Overal probability: 0.997883597884\n",
      "Probabilities: \n",
      "\tProb to arrive on time: 99.78835978835978\n",
      "Route: \n",
      "\tTake trip: 1914.TA.26-11-A-j19-1.27.R from 8591379 departing at 12:03:00 ------------> to 8591049 arriving at 12:24:00\n",
      "\tWalk from 8503000:0:10 ----to----> 8591379 for 7 minutes.\n",
      "9 routes found out of 10 best routes , if you want more please enter lower confidence"
     ]
    }
   ],
   "source": [
    "calculate_probabilities(current_time,best_routes,best_routes_print, confidence_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate the validity of this solution in one month (May 2019):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "matched_trips = spark.read.parquet(\"user/boecuego/matched_trips2.parquet\") \n",
    "mmm = matched_trips.filter(\"LENGTH(trips_stop_ids) > 15 AND sample_stop_ids != '%%' \").groupBy(\"trips_trip_id\").agg(F.collect_set(\"sbb_trip_id\").alias(\"possible_match\"))\n",
    "stop_matches = map(lambda row: row.asDict(), mmm.collect())\n",
    "dict_matches = {match['trips_trip_id']: match[\"possible_match\"] for match in stop_matches}\n",
    "sbb_inradius = spark.read.parquet(\"user/boecuego/sbb_inradius_weekdays_english.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "Except\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "Validation from the real past data:\n",
      "1.0\n",
      "**************************************************\n",
      "Time to run the validation code in seconds:\n",
      "1485.25756"
     ]
    }
   ],
   "source": [
    "t1=datetime.datetime.now()\n",
    "#Trips are from above result \n",
    "trips = [\"168.TA.26-12-A-j19-1.2.H\",\"20.TA.26-9-A-j19-1.2.H\"]\n",
    "departs = [\"12:23:00\",\"12:07:00\"]\n",
    "arrives = [\"12:29:00\",\"12:17:00\"]\n",
    "allz = zip(pairwise(trips),pairwise(departs),pairwise(arrives))\n",
    "dict_matches[\"32.TA.80-159-Y-j19-1.8.H\"] = [\"85:11:18844:001\"]\n",
    "\n",
    "days = [append_zero(str(x))+\".05.2019\" for x in range(1,30)]\n",
    "weekdayslst = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\"]\n",
    "\n",
    "weekdays = [x if datetime.datetime.strptime(x, '%d.%m.%Y').strftime('%A') in weekdayslst else \"\" for x in days]\n",
    "weekdays= filter(None,weekdays)\n",
    "\n",
    "catched_counter = 0\n",
    "counter = 0\n",
    "for day in weekdays:\n",
    "    print(\"*\"*50)\n",
    "    sbd = sbb_inradius.filter(F.col(\"date\")==day)\n",
    "    for trip,depart,arrive in reversed(allz):\n",
    "        print(\"- -\"*30)\n",
    "        ilkduraktankalkan = dict_matches[trip[1]]\n",
    "        ikinciduraktankalkan = dict_matches[trip[0]]\n",
    "        try:\n",
    "            ilkininin_ikinciye_varisi = sbd.filter((F.col(\"trip_id\").isin(ilkduraktankalkan)) \\\n",
    "                                                            &(F.substring(F.col(\"schedule_arrival\"),12,100) == arrive[1][:-3])).take(1)[0]\n",
    "            ikincinin_kalkisi = sbd.filter((F.col(\"trip_id\").isin(ikinciduraktankalkan)) \\\n",
    "                                                            &(F.substring(F.col(\"schedule_dep\"),12,100) == depart[0][:-3])).take(1)[0]\n",
    "\n",
    "            hm = ilkininin_ikinciye_varisi.arrival_time_real.split(':')\n",
    "            h = hm[0]\n",
    "            m = hm[1]\n",
    "            s = hm[2]\n",
    "            aa = datetime.datetime(2019,1,1,int(h),int(m),int(s),0)\n",
    "\n",
    "            hm = ikincinin_kalkisi.departure_time_real.split(':')\n",
    "            h = hm[0]\n",
    "            m = hm[1]\n",
    "            s = hm[2]\n",
    "            ad = datetime.datetime(2019,1,1,int(h),int(m),int(s),0)\n",
    "\n",
    "            window = (ad-aa).total_seconds()\n",
    "            #print(window/60)\n",
    "            counter +=1\n",
    "            if window >=0:\n",
    "                catched_counter+=1\n",
    "\n",
    "\n",
    "        except:\n",
    "            print \"Except\"\n",
    "t2=datetime.datetime.now()\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "print(\"Validation from the real past data:\")\n",
    "print(float(catched_counter)/float(counter))\n",
    "print(\"*\"*50)\n",
    "print(\"Time to run the validation code in seconds:\")\n",
    "print((t2-t1).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def schedule_planner(src, tgt, desired_arrival_time, confidence_level):\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    confidence_level=float(confidence_level)\n",
    "    \n",
    "    src_main = src.split(\":\")[0]\n",
    "    tgt_main = tgt.split(\":\")[0]\n",
    "    counter = 0\n",
    "    paths = []\n",
    "    for node in mygraph.nodes:\n",
    "        if src_main in node:\n",
    "            counter+=1\n",
    "\n",
    "    if counter > 1:\n",
    "        k = 5\n",
    "    else:\n",
    "        k=50\n",
    "    for node in mygraph.nodes:\n",
    "        if src_main in node:\n",
    "            p = k_shortest_paths(mygraph, node, tgt,k,weight = \"cost\")\n",
    "            for x in p:\n",
    "                paths.append(x)\n",
    "    no_adjecentwalk = remove_adjecent_walks(paths)\n",
    "    reversed_routes  = get_reverse_route(no_adjecentwalk)\n",
    "    \n",
    "    hm = desired_arrival_time.split(':')\n",
    "    h = hm[0]\n",
    "    m = hm[1]\n",
    "    #This is arrival_time\n",
    "    current_time = datetime.datetime(2019,1,1,int(h),int(m),0,0)\n",
    "    cut_off_time = current_time - datetime.timedelta(minutes=60)\n",
    "    \n",
    "    notPossible = datetime.datetime(1999,1,1,12,12,0,0)\n",
    "    good_routes,bad_routes,printouts = find_possible_routes(reversed_routes,current_time,cut_off_time)\n",
    "    \n",
    "    \"\"\"\n",
    "    for element in good_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element\n",
    "            \n",
    "    for element in bad_routes:\n",
    "        if type(element) == type([]):\n",
    "            for r in element:\n",
    "                print(r)\n",
    "        else:\n",
    "            print element\n",
    "    \"\"\"\n",
    "            \n",
    "    maxt = datetime.datetime(2019, 1, 1, 0,0)\n",
    "    index = -1\n",
    "    start_times  = []\n",
    "    for i,t in enumerate(good_routes):\n",
    "        if type(t) == type(datetime.datetime(2019, 1, 1, 11, 56)):\n",
    "            start_times.append((t,i))\n",
    "            if t > maxt:\n",
    "                maxt = t\n",
    "                index = i\n",
    "    start_times_reversed = sorted(start_times, key=lambda tup: tup[0],reverse=True)\n",
    "    best_routes = []\n",
    "    best_routes_print = [] \n",
    "\n",
    "    for time,index in start_times_reversed[:10]:\n",
    "        route = good_routes[index-1:index]\n",
    "        best_routes.append(route[0])\n",
    "        route_print = printouts[index/3]\n",
    "        best_routes_print.append(route_print)\n",
    "    \"\"\"\n",
    "    for r in best_routes_print:\n",
    "        print(\"******\"*10)\n",
    "        for x in r:\n",
    "            print x\n",
    "    for r in best_routes:\n",
    "        print(\"******\"*10)\n",
    "        for x in r:\n",
    "            print x\"\"\"\n",
    "    \n",
    "    print('*'*100)\n",
    "    print('PROBABILITIES')\n",
    "            \n",
    "    only_routes = good_routes[::3]\n",
    "    calculate_probabilities(current_time,best_routes,best_routes_print, confidence_level)\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    print('*'*100)\n",
    "    print(\"Running time:\", (end-start).total_seconds(), \"seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "def troll(src, tgt, desired_arrival_time, confidence_level):\n",
    "    # To push it to the spark\n",
    "    confidence_level = str(confidence_level)\n",
    "    \n",
    "    get_ipython().push(\"src\")\n",
    "    get_ipython().push(\"tgt\")\n",
    "    get_ipython().push(\"desired_arrival_time\")\n",
    "    get_ipython().push(\"confidence_level\")\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i src -t str -n src' , ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i tgt -t str -n tgt' , ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i desired_arrival_time -t str -n desired_arrival_time' , ' ')\n",
    "    get_ipython().run_cell_magic('send_to_spark','-i confidence_level -t str -n confidence_level' , ' ')\n",
    "\n",
    "    \n",
    "    get_ipython().run_cell_magic('spark', '', 'schedule_planner(src, tgt, desired_arrival_time, confidence_level)') \n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets \n",
    "func_handle = interact_manual(troll, src='8503000', tgt='8591049', desired_arrival_time='12:30',\n",
    "                                confidence_level=widgets.FloatSlider(min=0.0, max=1.0, step=1e-2, value=0.90))\n",
    "func_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
